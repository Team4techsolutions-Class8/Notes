Instructors name :  Calson 
Professional : DevOps Engineer specialist 

Partner name is ;  Ali Mohammed (Python)

TOOLS 

Communication 

+ Slack (Team communication) + Notifications (CI/CD)
+ Whatsapp (Announcement)
+ Telegram (All class recorded videos)
+ Draw.io 
+ Confluence ( Documentations )
+ Jira ( Ticketing)
+ Github (all our running notes)
+ Personal laptop or computer (highly recommended External Monitor )

-----------------------------------------------------
schedule 


Monday : 7pm EST - 10 pm EST (10 Mins break)
Tuesday: 7pm EST - 10 pm EST (10 Mins break) 
Thursday : 7pm EST - 10 pm EST (10 Mins break)

cloud : 4-5 months 
devops : 5-7 months including 1 month 


AWS / Linux , Bash , Terraform , ANSIBLE 
AZURE 



make up classes 

30 mins + Ask Questions from previous 


AFTER THE COURSE WHAT HAPPENS 

+ Linkedin profiling 
+ Resume profiling 
+ Show to apply for jobs ( AI)
+ Bootcamp session (1 month) - Techniques ) recorded videos 
+ Interview support  

---------------------------------
What type of OS you need for the course 

+ 13 inch 
+ Windows 
+ MacOs 

do not use a chromebook 


classes 

Monday : 






ROAD MAP FOR OUR DEVOPS PROGRAM 

+ Linux (Operating System)
+ Bash 
+ Git & Github 
+ AWS (Amazon Web Services) 
+ Terraform (IAC)
+ Ansible 
+ Jenkins 
+ Maven 
+ Docker 
+ Sonarqube 
+ Nexus
+ Kubernetes 
+ Helm Chats 
+ Prometheus and Grafana 
+ Nginx 
------------------------------

cloud engineers 
------------------

+ Linux (Operating System)
+ Bash 
+ Git & Github 
+ AWS (Amazon Web Services) 
+ Terraform (IAC)
+ Ansible 


9:17 pm est 

What is DevOps 
What is Cloud computing 



What is a software ?: Collection of programs,data , instructions that helps a computer or device to perform specific task 

1. System software e.g Windows , MacOs , Linux 
2. Application software (Mobile application and web application )

Business of Software development ----> 

Development Team ( Cross functional Team)



What is SDLC (Software Development Life Cycle)?

+ Plan (All development team)
+ Analysis 
+ Design ( UI/UX designers)
+ Implemenation ( Developers , DevOps , Cloud engineer )
+ Testing ( Q.A , DevOps )
+ Deploy (users can now use the software)
+ Maintenance 



Waterfall Methodology : ( Delivery delays ,Adaptability is not possible , customer is not engaged early in the development ,  )

100 m


Agile Methodology | scrum framework 

+ Sprint Planning 
+ Daily standups 
+ Sprint Retrospect 
+ Demo Session 


Agile Methodolgy ---> 

20 requirements : features 

sprint planning  ----> front end , sign up , send money via email 
2 weeks 
cloud engineers :  build infrastructure 




Schedule 

Monday : 7pm -10 pm est 
Tuesday: 7pm -10 pm est 
Thursday : 7pm -10 pm est 



Wednessday : 7Pm - 10 pm est 
Friday : 7pm - 10 pm est 
saturday : 10 am - 1 pm est 


iterative approach (scrum framework----> daily standup , Sprint planning , Sprint Retrospect  , Kanban )


Sprint Planning : 
Daily Standup : 


**Cloud Computing**

**Types of Computers** 


## Personal Computer (PC) : 
General Purpose e.g desktop ,Laptops, tablet 

## works stations : 
Work stations are high perfomance computers specialized task , graphic designing etc

## servers : 

 Servers are computers designed to provide services or resources to other computers over a network . eg web hosting , file storage 

## Mainframes :
   Large and power machines they are capable of handling large amount of data (information) and it can support many users at the same time .

## Super Computers : 
  These are fast and power computers 



**Computers can not function without an Operating System ( Computer Software)**

## Operating Systems

Windows Operating system   :  owned by Microsoft 
MacOs Operating System     :  Apple 
ChromeOS                   : Google
Linux Operating System     : Open source ( Technology ) - Linus Torvalds  ( Unix-like )



## Phones  operating system (software)

Android       : 
IOS    :
Chrome IOS 

## Open Source : 
You do not pay to use it , Source code is available for use or remodification 



# Vendors : they create and sell computers 

Toshiba :   Windows        
Lenovo :  windows 
Apple : MacOs
Dell : windows
Asus ; windows
Acer: windows
Samsung: windows
Panasonic : windows 
IBM : windows

`Third party Vendor` 

Walmart 
Best Buy 
The Source 



# Parts of a Computer 

## CPU : 
Central Processing Unit ; This is like the brain of your computer .Its performs instructions and calculations , Process information (data) and control operations of other things in the computers .

## Memory (RAM) : Random Access Memory :

A ram temporary stores information (data) , and information the cpu needs to quickly access 

## Storage Devices :
They permanently store information (data)
- HDD ( Hard Disk Drive) : 
- SDD ( Solid State Drive) : It uses flash memory to store information it offers faster read and write 
- Optical Drives : 

## Graphical Processing Unit 
This is responsible for graphic display on your computer 

## MotherBoad 

## Input Devices : It allows users to interact with the computer input data 
- Keyboard 
- Mouse 
- Touchpads 
- Touch screens 

## Ouput Devices : 
This displays data or information processed by the computer 

- Monitor 
- Printers 
- Speakers 

## Networking Devices : Modems , routers , network interfaces .





# CLOUD COMPUTING



`Traditional I.T infrastructure` : All I.T equipments like compute , networking , storages devices etc are stored in a physical location and managed by the I.T team 


`Cloud Computing` :  Cloud computing is an on demand delivery of I.T resources / services over the internet with a pay as you go pricing model instead 
of buying , owning and maintaining physical data centers and servers .


**Cloud Providers** 
These are the companies or orgnization that provides us the services to be able provision I.T infrastructure over the internet 

**Different Cloud Providers**

+ AWS ( Amazon Web services) 
+ Microsoft Azure 
+ GCP ( Google Cloud Platform)
+ IBM Cloud
+ Oracle Cloud 
+ Alibaba Cloud 
+ Digital Ocean 
+ Salesforce 



## Benefits of Cloud Computing 

+ **Cost Efficiency** : pay as you go model. you only pay for resources or services you use .This reduces upfront capital expenditure . (CAPEX )0
+ **Cloud is Scalable (out / in) and Flexible** : Cloud services provides on demand resource allocation , allowing businesses to easily scale up that is increase their resources , or you can decrease your resource at anytime .This beneficial to companies with fluctuating workload  
+ **Disaster Recovery and Business Continuity** : Cloud providers have robust / strong / resilient disaster recovery solutions .
+ **Security** : It has robust security services monitoring , WAF(web application firewall) , Networking 
+ **Maintenance and Upgrades are easy** : Most of the time the service provider is responsible for maintaining , managing and updating the infrastructure . This reliefs businesses from system maintenance , allows them to focus on the main activities . 
+ **Accessibility** : Cloud services can be accessed from anywhere with an internet connection, faciliating remote work and collaboration. 




**What is On premise ?**

This refers to the traditional model where companies or organisations manage their I.T infrastructure  e.g storages , networking hardwares , softwares etc in a physical location or onsite .The companies or organisations are responsible for maintaining the hardware, software , security and other aspects of the I.T environment .

**Benefits or advantages of having your I.T infrastructure on premise**

+ **Control and Customization on premise is good** . Organisation can customize and control their resources to meet different requirments 
+ **Security and compliance** : The cloud environment is secured but there are some strict complaince or regulations hinders companies to move to the cloud due to data concerns so having their data on premise provides more security to them.
+ **Performance** : On premise solutions can offer better perfomance to certain applications , as the data do not need to travel over the internet.This is good for applications which are sensitive to latency .
+ **Offline Accessibility** : On-premise solutions might not require internet connections for access , making them suitable for environments with unsuitable internet services .





 ## Deployment Models in Cloud Compting 

 This are specific environments where your services in the cloud are going to be hosted . 

1. **Public Cloud**

 Public cloud services are provided over the Internet and offered by third-party providers. These services are available to anyone who wants to use or purchase them. The provider owns, manages, and assumes all responsibility for the data centers and infrastructure. Examples include Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP).

----> Apartment Building  (Public Cloud) ------> tenants  (user) ------> rent apartment (cloud services) , gym , swim pool , massage parlour , (storage , servers, networking ), you dont own build  (infrastrure) you pay only when you use it . 


2. **Private Cloud** 

A private cloud is dedicated to a single organization and can be hosted internally or by a third-party provider. The infrastructure and services are maintained on a private network, and the hardware and software are dedicated solely to your organization. This model offers more control and security over your resources.

Single family home , everything within the property is yours ,control over modifications 


3. **Hybrid Cloud** 

Hybrid cloud combines public and private cloud elements, allowing data and applications to be shared between them. This model provides businesses with greater flexibility and more deployment options by allowing workloads to move between private and public clouds as computing needs and costs change.




**Types of Services offered by Cloud Providers ( aws , azure ,google cloud )**

1. `Infrastructure as a Service (IaaS)` :  These providers offer basic compute, storage, and networking resources on-demand, from the cloud, with scalable and elastic capabilities. Users have control over operating systems, storage, and deployed applications but do not manage the underlying cloud infrastructure. Examples of IaaS include physical and virtual servers, storage, and networking features.

2. `Platform as a service (PaaS)`: PaaS providers offer a cloud-based platform that allows developers to build, deploy, and manage applications without dealing with the underlying infrastructure. This service is geared towards software development, providing tools to support the complete web application lifecycle: building, testing, deploying, managing, and updating. PaaS includes services like web servers, development tools, and database management systems.

3. `Software as a service (Saas)`: Software as a Service (SaaS): SaaS providers offer software applications over the Internet, typically on a subscription basis. With SaaS, users can access and use software applications hosted on cloud infrastructure via web browsers without worrying about installation, maintenance, or infrastructure. This model includes applications like email, customer relationship management (CRM) systems, and collaboration tools.

4. `Infrastructure as code`(IAC) : This is a service where you can deploy your infrastructure using code . 



# AWS  (Amazon Web Services )

Amazon Web Services (AWS) is a comprehensive and widely adopted cloud platform offered by Amazon. Launched in 2006, AWS provides a variety of scalable and flexible cloud computing solutions to individuals, companies, and governments. Its extensive offering includes over 200 services available from data centers globally. These services are designed to help businesses scale and grow by providing powerful computing power, database storage, content delivery, and other functionality.

some services 

+ compute 
+ containers
+ database 
+ Networking 
+ Developer tools 
+ Machine Learning 



**What is a Data Center** : 

A data center is a centralized facility or a Physical location used by organizations or company to house/keep and manage their IT infrastructure, including servers, storage systems, networking equipment, and other critical components for storing, processing, and disseminating data and applications


AWS has its own data centers and they offer the cloud services over the internet from this phyiscal locations . customers , companies ,organisations store their data in this physical locations and they refer to their data centers as `AVAILABILITY ZONES` 



**What is a region** : 

In Amazon Web Services (AWS), a region is a geographical area where AWS has multiple data centers. Each region is designed to be completely isolated from other regions to ensure fault tolerance and stability. AWS operates multiple regions globally, and each region consists of multiple availability zones.

+ Each region is identified by a unique name (e.g., `virginia (us-east-1a , us-east-1b ,us-east-1c) , eu-west-1).canada is called ca-central-1` 
+ Regions are composed of multiple availability zones.
+ AWS services are typically deployed globally across multiple regions to provide redundancy and improve performance for users located in different geographic locations.





**WHAT IS AN AVAILABILITY ZONE (AZ)**

An availability zone (AZ) is one or more discrete data centers within a region. These availability zones are interconnected through low-latency links, but they are physically separated from each other. This physical separation is important for providing redundancy and resilience. If one availability zone within a region experiences a failure, the other availability zones in the same region can continue to operate independently, ensuring `high availability` and `fault tolerance` for applications and services hosted on AWS.

+ Availability zones are isolated from each other to minimize the impact of failures and improve `fault tolerance`.
+ They are connected through high-speed, low-latency links to enable synchronous replication and data transfer between availability zones.
+ Deploying applications across multiple availability zones within a region enhances fault tolerance and high availability by ensuring that failures in one availability zone do not impact the availability of applications hosted in other availability zones.

Region: us-east-1
availability zones : unique names of the data centers us-east-1a , us-east-1b , us-east-1c , us-east-1d





## Why did AWS introduce availability Zones?


+ **Fault Tolerance** : By distributing resources across multiple availability zones within the same region, applications can withstand failures in one availability zone without impacting availability. This redundancy ensures that if one availability zone experiences an outage, services can continue to operate from other availability zones.

+ **High Availability**: Applications deployed across multiple availability zones can achieve higher levels of availability because they are not dependent on a single data center or infrastructure. Even if one availability zone becomes unavailable due to maintenance, hardware failures, or other issues, the application can remain accessible from other availability zones.

+ **Improved Performance**: Availability zones are interconnected through low-latency links within the same region. This allows for synchronous data replication and enables applications to achieve low-latency communication between components deployed across different availability zones, enhancing overall performance.

+ **Scalability**: Availability zones provide scalability by allowing organizations to distribute workloads across multiple zones within a region. As demand for resources grows, organizations can easily scale their applications horizontally by adding more instances or services across different availability zones

+ **Disaster Recovery**: Availability zones facilitate disaster recovery by providing geographically separated infrastructure within the same region. Organizations can use availability zones to replicate data and resources across different zones, enabling them to quickly recover from disasters or regional outages without data loss or downtime.

+ **Compliance and Data Residency**: Some regulatory requirements mandate data residency within specific geographic regions. Availability zones allow organizations to ensure compliance with these requirements by deploying resources in designated regions while still benefiting from the redundancy and fault tolerance provided by multiple availability zones.



**Virtualization** 



**What is Virtualization** : Cloud providers like aws , azure , gcp are using the technology called Virtulization to offer I.T services over the internet. Virtualization in AWS (Amazon Web Services) or cloud computing refers to the practice of creating virtual instances of computing resources such as servers, storage, networks, and applications, which are then delivered over the internet. This allows users to access and utilize these resources on-demand without the need for physical hardware infrastructure.

In AWS, virtualization is achieved through technologies like Amazon Elastic Compute Cloud (EC2) (computers or servers you can create on aws) for virtual servers, Amazon Simple Storage Service (S3) for virtual storage, and Amazon Virtual Private Cloud (VPC) for virtual networks. These services abstract the underlying hardware and provide users with scalable and flexible computing resources that can be easily provisioned and managed via a web interface or APIs.


**Virtual Machines (VMs)**: A VM is a software-based emulation or imitation of a physical computer that runs its own operating system and applications. Virtual Machines are created and managed by the hypervisor and can be provisioned, cloned, migrated, and scaled as needed.





## Virtualization Types:**

### Hardware Virtualization ; 

Hardware virtualization involves creating a virtual machine that acts like a real computer with an operating system. This type is often used to consolidate multiple physical machines into fewer hardware servers, saving space and resources.


**Full Virtualization** : In full virtualization, the guest operating system (This is the computer you will create from the main computer (host computer)  runs unmodified on the virtual machine, while the hypervisor handles hardware emulation and resource management.

**Para-virtualization**: In para-virtualization, the guest operating system is modified to be aware of the virtualization layer, which can improve performance and efficiency.

**Hardware-assisted Virtualization**: Modern processors include hardware features that accelerate virtualization, such as Intel VT-x and AMD-V, which enhance the performance and security of virtualized environments.


**Types of Hypervisors** 

Bare-Metal Hypervisors: This are physical hardware 
+ VMware vSphere/ESXi, Microsoft Hyper-V, and Citrix XenServer.

Hypervisors (Hosted Hypervisors)  : Oracle VirtualBox, VMware Workstation, and Parallels Desktop.




+ Citrix Xen (Xenserver) : An Opensource Hypervisor ( software)
+ Vmware ESXI 
+ Microsoft Hyper-V  : windows 
+ KVM ( Kernel based virtual machine)




## Virtualization Benefits:

1.  **Cost Efficiency** : 

+ Virtualization allows multiple virtual systems to run on a single physical system, reducing the need for multiple hardware units. This consolidation significantly reduces hardware costs.

+ It serves energy because eliminates physical servers that could have been consuming electricity or energy . It lowers operating costs but also helps in reducing the carbon footprint of data centers.

2.  **It reduces resource Utilization**: 

+ Efficient Use of Hardware: Virtualization allows multiple virtual machines (VMs) to run on a single physical machine, maximizing hardware utilization.
+ Reduced Hardware Costs: By consolidating servers, organizations can reduce the need for physical hardware, leading to cost savings.

3. **Scalability and Flexibility** : Virtual environments make it easy to scale up or down based on demand . that is increase resources for example computer or servers based on demand and easy provision (easy to create)

4. **Isolation and Security** 

+ Enhanced Isolation: Each VM(virtual machine ) operates independently, providing isolation between applications and services, which enhances security.
+ Controlled Environment: Virtualization allows for controlled environments for testing and development without impacting the production environment.

5. **Disaster Recovery and High Availability**

+ Simplified Backup and Recovery: Virtualization simplifies the backup process by enabling snapshots of VMs, making disaster recovery more efficient.
+ High Availability: VMs can be moved between physical servers without downtime, ensuring continuous availability.



**Take note : AWS services | resources are categorized based on their scope and reach within the aws Infrastructure or ecosystem** 

1. **Regional Services* : 
These are services that operates within a specific aws region .

+ They are confied to a specific geographical area of a single AWS region which may contain Multiple availability zones 
exmples of regional services can be `VPC, Amazon EC2 , Amazon RDS , AWS lambda function , SNS etc 
**Usage*: These services are used when data residency and latency requirements are tied to a specific geographic location.


2. **Global Services*: 
Services that operate across multiple AWS regions simultaneously.

+ They are designed to provide a unified view and functionality regardless of the geographic region.
+ Examples of these services in aws IAM (Identity and access management),Route53 , Aws cloudfront , S3 buckets 

3. **Zonal Services*

+ Services that operate within a specific availability zone within an AWS region.
+  They are limited to a single availability zone, providing high availability and fault isolation.
+ Examples of these services EBS Volume (Elastic Block Store), EC2 Instances .
+ These services are used for applications that require low latency within a single availability zone or need fault isolation within a region.

| Service Type    | Scope                          | Examples                                                      |
|-----------------|-------------------------------|---------------------------------------------------------------|
| **Regional**    | Operate within a single region | Amazon VPC, Amazon EC2, Amazon RDS, AWS Lambda                |
| **Global**      | Operate across multiple regions| Amazon Route 53, AWS CloudFront, AWS IAM, Amazon S3 (global)  |
| **Zonal**       | Operate within a single AZ     | Amazon EC2 instances, Amazon EBS volumes                      |



FIRST AWS SERVICE 

# IAM : Identity and Access Management 

+ This is a Global Service in AWS 
+ This is one of the core services provided by AWS 
+ We can classify it under security 

**Definition** : IAM stands for Identity and Access Management, and it's a core service provided by Amazon Web Services (AWS) that enables you to manage access to AWS resources securely. IAM allows you to control who can access your AWS resources (authentication) and what actions they can perform on those resources (authorization).



**Key Features of IAM**  

+ User Management : You can create and manage users and groups , assign permissions to allow access to resources using IAM . 

**User** : Represents a person or service that interacts with AWS . Users have permanent credentials and are assigned permissions through policies.

User can have access or use aws using two methods .

+ AWS management console (Graphical User Interface)
+ Terminal (CLI : Command line interface)


To setup commandline interface as a user using the commandline 

Step 1 :

Install the aws cli on your computer . If you have a MacOs , on the launch pad open and search terminal , for windows on the start open and search `command prompt` or `powershell` 

step2 

Install aws cli 
follow this documentation to install base on your Operating system : https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html 

Step 3 

The user you created click on the user and under the user click on security credentials.
Look for Access Keys and click on create access keys 
after creating the access keys configure the keys by using the following commands on the CLI 

step3 

type : `aws configure` 

and follow prompt and fill the following information 
that is the access and secret key including your default region name and output format which will always json 

AWS Access Key ID []: AKIA47CR2IN2VU5MUERN
AWS Secret Access Key []: yn/ETZE9J8QVNlOdijXPBc7KrtidyxarGL7SdW7V
Default region name []: ca-central-1
Default output format []: json


When you create user on aws we refer them as `IAM user` 
Two ways you can login on your aws account using the console :  `Root user`(this is user who created the aws acct) or Iam user 
when you create yOur aws account it comes with the root user by default 
As security best practise it is highly recommended that you should never login with your root user instead use IAM users

**What are security best practises** 

+ best practise aws recommends that all access keys be rotated every 90 days 
+ As security best practise do not use the root account instead create an IAM user and login with the user . 




## WHY SHOULD YOU USE `IAM USER` INSTEAD OF `ROOT USER` IN AWS ? 

+ **Principle of Least Previllage** : Root users have a lot of unrestricted access to all resources on your aws account which escalated permission if there was security breach it will impact your account and resources . 

USE IAM USER where you can grant them permission to carry out specific task , if this user was compromised there will not be too much impact in your account and resources . 

+ **Reduces security Risk** : 
+ Root account is high risk , if the root user is compromised by an attacker the will have access to all resources 
+ Iam users have lower risk : If this user was compromised the will be damaged only on the resources the user had access to 

+ **Accountability & Auditing** :  

+ Using the rootuser can lead to Lack of accountability : Actions performed by the root user are harder to attribute to a specific individual, making it difficult to track changes and identify issues.
+ On the other hand when you use an IAM user you can monitor individual actions , logings and you audit all these through a service in aws called Cloudtrail . 

+ **Operational Safety** : 

+ using the root user can lead to accidental misused whereby you can unintentionally delete critical resources .Using it everyday leads to risk 
+ whereas using iam users have controlled access and permissions reducing the risk of accidental delete of resources .



**Group** :
`What is a group` ? : A group is a collection of users, you can use groups to assign permissions to `multiple users simultaneously` 

+ in a company you can have different groups within the development team e.g ( DevOps , Cloud Engineers , Quality Assurance engineers , UI/UX etc)
+ its a good practise to assigned permission to users in groups as much as possible . 
+ When you give a permission to a group , all users under that group automatically inherit the permissions . 
+ When creating a group you can assign existing users to the group and permissions & you can also create an empty group 
+ a user can be in multiple groups at the same time 




## Benefits of Managing Permissions Using Groups in `IAM `

1.  **It simplifies permission management** : 
- you have a centralized control of permissions this is possible because by attaching a policy to the group it automatically applies to all the users in the group . 
- Assigning users to groups and attaching a permission to the group makes sure there is consistency across all users.All users wil have the same permissions unlike if i wanted to manually assign permissions to users you could make a mistake . 

2. **Managing users using Groups makes it Scalable**
- Its easy to Onboard ,When a new user is added to the company adding them to the group speeds up their onboarding process
- easy to offboarding (temporary when a user is on vacation or permanently if the user is no longer employed) when someone is leaving removing them from the group revokes their permission 

3. **Enhanced security** : 
- Using groups will lead to enhancing security by following the principle of least previllage accessed
_ It makes it easy for auditing . Auding group is easier than auting individual users 

6. Improved Efficiency ( time saving )







# Policies : 

Its a Json Document that defines permissions.This document specifies which actions are allowed or denied on which resource or resources and under what conditions a user can manage them . 
+ This is you authorizing a group , user or service to manage your resource or resources in aws . 


Polices are simply permissions , but this permissions are being granted or denied using what we call a json document .In this document we defined if the permission should allow a user or group of users to carryout a spefic action on a resource or resources 



**Different Types of Policies**

# MANAGED POLICIES

1. **AWS managed Policies** : 

+ These are policies that comes with your aws account by default 
+ AWS manages these policies that is why its called AWS managed 
+ With aws managed policies you can not delete the policy and you also can not modify the policy
+  You can attached the policy to multiple users and groups or role 

2. **Customer Managed Policies**: 

+ This is a policy that is created by the user and the user manages the policy 
+ You can attached the policy to multiple users and groups or role 
+ You can reuse the policy and you can delete or modify the policy 


###############################

**Inline Policy**: 

+ This is a policy created by the user and managed by the user 
+ You can only directly attached it to a single user , group or role 
+ They provide specific permissions to a particular entity ( user , group , service) which makes it not resuable ) 
+ Use inline policies only if you think no other user will need that permission in the future because you cant reuse the policy 




DIFFERENCE BETWEEN MANAGED POLICES AND INLINE POLICIES

| Feature                   | AWS Managed Policies                    | Customer Managed Policies               | Inline Policies                          |
|---------------------------|------------------------------------------|-----------------------------------------|------------------------------------------|
| **Definition**            | Predefined policies created and managed by AWS. | Policies created and managed by the customer. | Policies embedded directly into a user, group, or role. |
| **Management**            | Managed by AWS; automatically updated by AWS to include new services and actions. | Managed by the customer; allows for customization to meet specific needs. | Managed directly within the user, group, or role it is attached to. |
| **Reusability**           | Can be attached to multiple users, groups, or roles across the account. | Can be reused across multiple users, groups, or roles within the account. | Specific to a single user, group, or role and cannot be reused. |
| **Modification**          | Cannot be modified by customers.         | Can be created, updated, and deleted by customers. | Can be edited directly within the user, group, or role it is attached to. |
| **Examples**              | `AdministratorAccess`, `AmazonS3ReadOnlyAccess` | Custom policies created by the customer. | Inline policies specific to a particular entity. |
| **Use Case**              | Common permissions needed across many AWS accounts. | Specific needs not covered by AWS managed policies. | Specific, non-reusable permissions tailored to individual users, groups, or roles. |
| **Complexity**            | Simple to use; AWS handles updates and maintenance. | Requires customer to manage and update as needed. | Simple to assign but can become complex to manage at scale. |
| **Policy Scope**          | Broad and generic to cover common use cases. | Can be tailored to precise needs and security requirements. | Very specific to the needs of a single entity. |
| **Best Practices**        | Use for general permissions where AWS managed policies suffice. | Use for more granular control where specific permissions are needed. | Use sparingly for specific cases where managed policies are not suitable. |

recommendation : If you using permissionsets to grant access use more inline policies 
if using IAM : use more of customer managed policies 


# Policies : 

Its a Json Document that defines permissions.This document specify which actions are allowed or denied on which resource or resources and under what conditions a user can manage them . 
+ This is used to authorize groups , user or service to manage your resource or resources within your aws account. 


**Example of a Json Policy**  the full meaning of json is -->  Javascript object notation   

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:ListBucket",
                "s3:DeleteObject"
            ],
            "Resource": [

            ]
        }
    ]
}

--------


wildcard 


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "s3:*",
            "Resource": [
                "arn:aws:s3:::your-bucket-name",
                "arn:aws:s3:::your-bucket-name/*"
            ]
        },
        {
            "sid" :denys3actions
            "Effect": "Deny",
            "Action": "s3:*",
            "Resource": [
                "arn:aws:s3:::your-bucket-name",
                "arn:aws:s3:::your-bucket-name/*"
            ],
            "Condition": {
                "NotIpAddress": {
                    "aws:SourceIp": "192.0.2.0/24"
                }
            }
        }
    ]
}





**Version**: This is the policy language version you are using 
**Statement** ; A policy can have one or more statements and each statement describe a specific permission 
  + **Effect** : The effect specifies if the policy is allowing or denying access ( Allow or Deny) to access a service or resource
  + **Action** This are actual actions/permissions/ task a user can carryout on the resource they have been allowed to have access to 
**resource** : This is a service or object or resource to which the actions are applied to
**Sid** : This is a description used to distinguish different statement policies
**condition** : This defines when a policy is in effect. These are optional 



HANDS ON 
Creating an IAM policy from scratch 

There are two policy editors we can use to create IAM policies

1. Visual Policy Editor
2. Json Policy Editor 



**RECAP : QUESTIONS AND ANSWERS** 

policies are permissions that can be assigned to users or groups 
gives access to someone to do a task in aws 
its json that accepts or deny users to do some specific task 


+ Inline Policies 
+ Managed Policies (AWS managed policies and Customer managed policies)


Best Practise 

+ Follow Principle of Least Previllage 
+ Avoid using the root user and instead user IAM USERS 
+ Access keys should be rotated maximum 90 days 
+ Temporary Suspend unused keys especially if an employee is on vacation for 2 + weeks 
+ Used groups as much as possible when managing identity and access management 
+ Avoid using wild card 
+ Set up strong password policies for your iam users this prevents poor passwords which could lead to compromise 
+ Set up IAM user password rotation 90days preventing IAM users from reusing the same password 



**IAM ROLES**

**IAM ROLES** : Its a set of permissions an AWS Services ,User , Groups assume temporary which allows or deny which actions they can perform on a resource or resources. 
Roles are good for resource to resource authorization. This means if you want two resource to share data for authorization its highly recommended to use IAM ROLES instead of IAM POLICIES.IAM roles unlike an IAM USER that has access keys , password attached to them IAM roles do not have any long term credentials attached to them

Iam policies are longlived method of authorizing someone in ur aws account 
iam roles on the other are short live permissions to authorized a user ,groups in ur account 

IAM ROLE can be associated to a user , resource or used for cross accoubnt permissions 


+ Temporary Credentials 
+ Flexible 
+ You can delegate roles to users , service etc without long term credentials


`Trusted Entity` : A trusted entity is who is going to assume or use the role. 

entities

+ AWS service (ec2 , s3 , cloudfront ,cloudwatch)
+ AWS Account (891377304437 )
+ Web identity 
+ SAML 2.0 Federation 
+ Custom Trust Policy 



**Trust Policy** : This policy defines who can assume / use the role .
In the trust policy the service that assumes the role is what we call the `principal`  

example of a trust policy 

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "ec2.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}


**Understanding the Trust Policy for IAM ROLES** 

`version` : policy version 
`statement`: This is the permissions define in the trust policy  
`Effect` : This grants permission to allow or deny a user or service to assume or use the role 
`Principal` : This is simply the service ,user , account who will assume or use the role 
`action` : This defines the action the trust policy wants the entity to do 


** DIFFERENCE BETWEEN IAM ROLE AND IAM POLICY **    


| Feature               | IAM Role                                                                 | IAM Policy                                                                 |
|-----------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------------|
| **Identity**          | An IAM role is an AWS identity with specific permissions.                | An IAM policy is a JSON document that defines permissions.                |
| **Usage**             | Intended for use by applications, AWS services, or users needing temporary access. | Policies are attached to IAM identities (users, groups, roles) to grant permissions. |
| **Trust Policy**      | Has a trust policy that defines who can assume /use the role.                 | N/A                                                                       |
| **Credentials**       | Provides temporary security credentials when assumed.                   | N/A                                                                       |
| **Attachment**        | Roles are not attached to users but assumed by entities needing the role. | Policies can be attached to users, groups, or roles.                      |
| **Types**             | N/A                                                                      | Managed Policies and Inline Policies                                      |
| **Permissions**       | Permissions are defined by the role's attached policies.                 | Defines what actions are allowed or denied on specified resources.        |



`Identity Provider`

Identity Providers (IdPs) in IAM refer to external systems that allow users to authenticate to AWS. AWS Identity and Access Management (IAM) supports several ways to establish trust with identity providers for authentication, including using web identity federation and Active Directory. This enables users to log in to AWS using credentials from an external provider, such as Google, Facebook, or corporate directories.

we have SAML 
OPENID CONNECT 


**Account Settings**
This will help you to Configure the password requirements for the IAM users
Password policy : 



# ACCESS ANALYZER  : 

This is a feature in IAM that helps you to identify resources in your aws accounts and organizations such as Amazon S3 buckets or IAM roles, that are shared with an external entity. It uses logic-based reasoning to analyze resource-based policies, IAM policies, and other access control mechanisms to provide a detailed report on resources that are accessible from outside your AWS account.

+ `Resource Analysis`: It identify the resources shared with external entities 
+ `Policy validation`: It checks your policies to ensure you are not granting unintended access & it provides recommendations to enhanced your security posture in your aws account  
+ `Continous Monitoring` : Monitors changes in your resource policies & updates the findings  
+ `Displays detailed findings` : it displays information about the external entity , the resource which helps you to understand the level of access the external entity has 


`external access`: Analyzing external entities(services) having access to your aws resources 
`unused access` : This are access that have not been active for a number of days . 


## Benefits of Using Access Analyzer

+ It Improves security by helping you to identify any access with external entities and helps you to mitigate or prevent any unintended access to your aws resources 
+ It helps you to identify any unintended resource sharing 
+ It ensures that your resource policies are correctly configured and do not expose your resources by giving you recommendations 
+ It helps in compliance , it provides information or insights into access configurations (SOC2)
+ It generates IAM policies based on Access activity 


## Credential Report 

Credential Report gives you an overview of your iam users and their settings , activities , access levels etc . some of the things you will see in the report
+ User information 
+ Password information 
+ Access Key information 
+ MFA (Multi Factor Authentication) information 
+ Security access information 

**WHY IS IT IMPORTANT TO USE CRENDENTIAL REPORT**

+ It helps you in auditing your iam users this helps to understand user credentials and usage 
+ It helps in securitty monitoring it gives you an overview of your iam users security credentials statuses allowing you to proactive protect your aws account . e.g you can see if iam users have not set up MFA (Multi factor authentication). 
+ It helps you with user management . it helps you to identify any security risk especially when there is outdated credentials or inactive users you can simply see and remove them . 


**MFA**

MFA (Multi Factor Authentication): In IAM MFA'S adds an extra layer of security for users who have access to your aws account . 

+ Increased security because a user after putting in their username and password will still need to confirm the MFA code before it allows them to login . Which means if their password was compromised the attacker will need to have access to their device e.g cell phone before they can login into the account 

+Some Supported Devices you can used for MFA are ( Virtual MFA device e.g google authenticator, Microsoft Authenticator, Duo Mobile, or Authy app  )


summary of iam 


So far we have been learning on how to manage and setup iam users within our aws console 
but we can also create iam policies ,users , user groups through the commandline 



all aws commands starts with `aws` this helps to send api calls to your aws account . 

aws = command to interact with aws api 
resource = s3 ,ec2 , iam ,cloudformation 
action = what you need e.g ls means list 


**AWS IAM User Management Commands**:

Create User
`aws iam create-user --user-name MyUser`
Use Case: Create a new IAM user named MyUser.

Delete User

`aws iam delete-user --user-name MyUser`
Use Case: Delete the IAM user named MyUser.

List users 
`aws iam list-users`
Use Case: List all IAM users in the account.


Update User
`aws iam update-user --user-name MyUser --new-user-name NewUserName`
Use Case: Rename an existing user from MyUser to NewUserName.
Get User


`aws iam get-user --user-name MyUser`
Use Case: Retrieve details for a specific user named MyUser.
Create Login Profile


`aws iam create-login-profile --user-name MyUser --password MyPassword`
Use Case: Create a password for the IAM user MyUser to allow console access.


**Group Management Commands:**

Create Group
`aws iam create-group --group-name MyGroup`
Use Case: Create a new IAM group named MyGroup.


Delete Group
`aws iam delete-group --group-name MyGroup`
Use Case: Delete the IAM group named MyGroup.


List Groups
`aws iam list-groups`
Use Case: List all IAM groups in the account.
Add User to Group

see more command below on the table 

### User Management Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam create-user --user-name MyUser` | Create a new IAM user | Create a new IAM user named `MyUser`. |
| `aws iam delete-user --user-name MyUser` | Delete an IAM user | Delete the IAM user named `MyUser`. |
| `aws iam list-users` | List all IAM users | List all IAM users in the account. |
| `aws iam update-user --user-name MyUser --new-user-name NewUserName` | Update an IAM user's name | Rename an existing user from `MyUser` to `NewUserName`. |
| `aws iam get-user --user-name MyUser` | Get details for an IAM user | Retrieve details for a specific user named `MyUser`. |
| `aws iam create-login-profile --user-name MyUser --password MyPassword` | Create a login profile for an IAM user | Create a password for the IAM user `MyUser` to allow console access. |

### Group Management Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam create-group --group-name MyGroup` | Create a new IAM group | Create a new IAM group named `MyGroup`. |
| `aws iam delete-group --group-name MyGroup` | Delete an IAM group | Delete the IAM group named `MyGroup`. |
| `aws iam list-groups` | List all IAM groups | List all IAM groups in the account. |
| `aws iam add-user-to-group --user-name MyUser --group-name MyGroup` | Add a user to a group | Add the user `MyUser` to the group `MyGroup`. |
| `aws iam remove-user-from-group --user-name MyUser --group-name MyGroup` | Remove a user from a group | Remove the user `MyUser` from the group `MyGroup`. |

### Role Management Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam create-role --role-name MyRole --assume-role-policy-document file://policy.json` | Create a new IAM role | Create a new IAM role named `MyRole` with the specified trust policy. |
| `aws iam delete-role --role-name MyRole` | Delete an IAM role | Delete the IAM role named `MyRole`. |
| `aws iam list-roles` | List all IAM roles | List all IAM roles in the account. |
| `aws iam attach-role-policy --role-name MyRole --policy-arn arn:aws:iam::aws:policy/MyPolicy` | Attach a policy to a role | Attach a managed policy to the role `MyRole`. |
| `aws iam detach-role-policy --role-name MyRole --policy-arn arn:aws:iam::aws:policy/MyPolicy` | Detach a policy from a role | Detach a managed policy from the role `MyRole`. |

### Policy Management Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam create-policy --policy-name MyPolicy --policy-document file://policy.json` | Create a new IAM policy | Create a new IAM policy named `MyPolicy` with the specified policy document. |
| `aws iam delete-policy --policy-arn arn:aws:iam::account-id:policy/MyPolicy` | Delete an IAM policy | Delete the IAM policy named `MyPolicy`. |
| `aws iam list-policies` | List all IAM policies | List all IAM policies in the account. |
| `aws iam attach-user-policy --user-name MyUser --policy-arn arn:aws:iam::aws:policy/MyPolicy` | Attach a policy to a user | Attach a managed policy to the user `MyUser`. |
| `aws iam detach-user-policy --user-name MyUser --policy-arn arn:aws:iam::aws:policy/MyPolicy` | Detach a policy from a user | Detach a managed policy from the user `MyUser`. |

### MFA Management Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam create-virtual-mfa-device --virtual-mfa-device-name MyMFADevice` | Create a new virtual MFA device | Create a new virtual MFA device. |
| `aws iam enable-mfa-device --user-name MyUser --serial-number arn:aws:iam::account-id:mfa/MyMFADevice --authentication-code-1 123456 --authentication-code-2 654321` | Enable an MFA device for a user | Enable the MFA device for the user `MyUser`. |
| `aws iam deactivate-mfa-device --user-name MyUser --serial-number arn:aws:iam::account-id:mfa/MyMFADevice` | Deactivate an MFA device for a user | Deactivate the MFA device for the user `MyUser`. |
| `aws iam delete-virtual-mfa-device --serial-number arn:aws:iam::account-id:mfa/MyMFADevice` | Delete a virtual MFA device | Delete a virtual MFA device. |

### Credential Report Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam generate-credential-report` | Generate a credential report | Generate a report detailing the status of user credentials. |
| `aws iam get-credential-report --output text --query Content | base64 -d > credential_report.csv` | Retrieve the credential report | Retrieve and save the credential report in CSV format. |




**IAM BEST PRACTISES**

+ `Follow Principle of Least Previllage` :Grant only the minimum permissions necessary for a user, role, or service to perform their job.
+ `Avoid using the root user and instead user IAM USERS` :Avoid using the root user for everyday tasks. Create IAM users with only the required permissions and assign roles to the root account for administrative tasks when necessary
+ `Access keys should be rotated maximum 90 days`: Regularly rotate access keys for IAM users and ensure they are not left inactive. Implement a policy to revoke unused access keys. For services like AWS CLI, use roles or temporary credentials instead of long-lived keys. 
+ Temporary Suspend unused keys especially if an employee is on vacation for 2 weeks 
+ `Used groups as much as possible to manage user permissions`: Instead of assigning permissions directly to IAM users, create IAM groups and assign permissions to the group. Then, add users to the appropriate group(s). This simplifies permission management and ensures consistency.
+ `Avoid using wild card` 
+ `Set up strong password policies for your iam users this prevents poor passwords which could lead to compromise 
+ Set up IAM user password rotation 90days preventing IAM users from reusing the same password 
+ Enable Multi-Factor Authentication (MFA):Enable MFA on the root account and any other IAM user accounts . This adds an extra layer of protection to prevent unauthorized access.
+ `Use more of Managed Policies (Cusotmer Managed) where possible` : AWS provides predefined managed policies that cover common use cases and best practices. Use these policies when possible instead of creating custom policies, unless you have very specific requirements.
+ `Monitor and Audit IAM Activity`: Enable CloudTrail to log IAM activity and regularly review logs to track who accessed what resources and when. Use this data for security auditing and for identifying suspicious activities.Also use features like Credential report and Access Analyzers (external or unused accesses)





#################################
# EC2 INSTANCES  Elactic COMPUTE 
#################################

EC2 - Elastic Compute Cloud
EC2 - Core / Main  services in AWS 
EC2 - Regional Service / Zonal Service 
EC2 instance is the general name of how aws identifies its Compute its virtual machines. 


**PURCHASING OPTION**
Free Tier :   Free Trial 

750 hours 

**ON DEMAND INSTANCES** : You will create a virtual machine or EC2 instance without any long term commitment, no upfront payment.You pay as you go and its flexible.This is purchasing option is `expensive` when compared to other purchasing options in aws . 

## Use case : 
+ application deployments especially applications with unpredictable workload 
+ Developments , Testing , production env etc 

**RESERVED INSTANCES** : It gives you signifcant savings discounts for up to %75 when compared to on demand instances. you can reserve an instance for 1 year , 2 years or 3 years in particular aws region , and a specific instance type . You can pay upfront , you can make partial payment or no upfront payment . Its not flexible like on demand instances. 

+ *USE CASE* : Good for applications with predictable workload 

**SPOT INSTANCES** : This are instances that aws allows you to place a bid to unused EC2 capacity and you will be able to run the instances as long as your bid exceeds the current spot price . Spot instances and thier prices are set up by AWS and these prices will fluctuate based on demand for the instances . AWS offers 90 % off  demand rates .
+ However even though its cost savings , cheaper the instance can be interrupted by AWS with a two minute warning when the capacity is needed for on demand instances . 

## USE CASE : Temporary task , applications that are flexible , fault tolerence , handle interruptions ,data analysis . 


## SAVING PLANS :
It offers significant savings and more flexible than reserved instance .You can reserved an instance for 1-3 years .You will commit to consistent amount of usage ($/hr) for 1 year or 3 year term . $0.53/hr .With the saving plans it offers lower price on EC2 instances regardless of the instance family , size , OS , tenancy , aws region in exhange the above commitment of usage.

+ Compute Savings Plan : 
+ EC2 instance saving plan : Offers lower prices on a specific instance family in a choosen aws region . 


## DEDICATED HOST:  
A dedicated host is a pyshical server with EC2 instance capacity fully dedicated to your use . Dedicated host allows you to use software licenses and can help you meet compliance requirements. Unlike  EC2  that will run on a shared hardware , dedicate hosts gives visiblity and control how instances are placed on the server. 
You can install your own software like windows servers , Linux  etc

`Use Case` : scenario where you what EC2 instances to be physically isolated at the host hardware level from instances that belong to other AWS accounts.When a company is subject meet certain policies or regulatory . 

Pricing : varies based on instance family , region and payment option you choose . You can pay to own the entire host which is expensive but on the other hand  dedicated instances are cost effective 



## DEDICATED INSTANCE 
 A dedicated instance are EC2 instances that are physically isolated at the host hardware level from instances that belong to other aws account (customers). However unlike dedicated hosts you do not have control over instance placements and cannot use your software existing license 

 + Benefit is to enhance security by ensuring strict isolation. 
 + It can be for regulations , compliance 


## CAPACITY RESERVATIONS :
This is when you can reserve a number of ec2 instances in a specific availability zone for any duration . 
USE CASE : This is vital when you always want the capacity needed for your critical application .
The price here is charged based on the on demand rates . 


Summary 

+ `Flexibility` :  On demand instances
+ `Predictability , Long term usage` : Reserved instances and Saving plans 
+ `High cost savings and flexible workloads` : Spot instances 
+ `Compliance and License requirements` : Dedicated Hosts or Dedicated instances
+ `Guaranteed Capacity` :  Capacity reservations 


# Interview Question 

1. Can you tell me how you have helped your company to implement cost optimization in the cloud e.g AWS 



**INSTANCE TYPE** 



+ **General Purposes** : General purpose instances provide a balance of compute, memory, and networking resources. These instances are ideal for applications that use these resources in equal proportions, such as web servers and code repositories. e.g T2 - t2 medium , micro etc 

+ **Compute Optimized** : Compute optimized instances are designed for compute intensive applications that benefit from high performance processors. These instances are ideal for batch processing workloads, media transcoding, high performance web servers, high performance computing (HPC), scientific modeling, dedicated gaming servers, ad server engines, and machine 
learning inference.  e.g C family 

+ **Memory Optimized** : Memory optimized instances are designed to deliver fast performance for workloads that process large datasets in memory. e.g R family 

+ **Storage Optimized** : Storage optimized instances are designed for workloads that require high, sequential read and write access to very large data sets on local storage. They are optimized to deliver tens of thousands of low-latency, random I/O operations per second (IOPS) to applications. e.g D FAMILY 

+ **Accelerated Computer instance**: Ac
celerated computing instances use hardware accelerators, or co-processors, to perform functions, such as floating point number calculations, graphics processing, or data pattern matching, more efficiently than is possible in software running on CPUs.

+ **High Perfomance Instances** : High-performance computing instances are purpose built to offer the best price performance for running HPC workloads at scale on AWS. These instances are ideal for applications that benefit from high-performance processors, such as large, complex simulations and deep learning workloads.e.g  (HPC)


reference :  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html


EC2 ---> MAC 
OS --> Linux , Microsoft OS , MacOs 
Different Types of Linux Operating Systems (`Linux Distributions or Linux Flavours`  ) --->(Ubuntu , Redhat , CentOs , Fedora , Amazon Linux etc )



########################
# Thursday 13 ,2025
########################

`Launch instances` in aws 
`Provisioned` Ec2 instances in aws 
`Instanstiate` 


##  AMI : Amazon Machine Image 

Its a pre-configured template (file) that contains software configurations (Operating System, Application server , applications and related configurations or dependencies) required to launch a virtual machine ( EC2-Instance )

## WHAT YOU SHOULD KNOW WHEN IT COMES TO AMI 

+ Template for your ec2 instance. This is the Pre-configured OS(operating system) or application for your virtual machine 
+ Users can customize AMI to match their specific needs and they can also modify existing AMI 
+ You can use a single AMI to create multiple ec2- instances with the same AMI configurations
+ There are Private (only the user who owns the AMI can see and use it ) and Public AMI (anyone on the internet (aws) can see and use it) 
+ AMI can be build from a snapshoot 
+ AMI are built in specific regions (regional) , and can be copied from one region to another and from one aws to another 
+ You can get AMI from AWS market place


+ Private AMI 
+ Custom default AMI from AWS 
+ AMI from AWS market place ( Public AMI)


**KEY PAIRS** 

What are the different ways a user can login into an ec2 instance ? 
a use can be authenticated to login into an ec2 instance using the following methods : 

+ Key Pairs 
+ Password 

The process of logging is using key pairs and password is called SSH (Secure Shell )

+ A key pair consist of a public and a private key .The Public key is stored in AWS .While the private key is stored or kept by the user .Key pairs are primarilly used for SSH Access. 
+ An ec2 instance without a keypair you wont be able to SSH into the server but you can still login to the server using different methods . 
+ You can use a single keypair on several ec2 instances and you can use different keypairs on different ec2 instances 
+ When naming a keypair use a single name , do not put spaces in the naming convention 
+ When you create a keypair for the first time it goes to your downloads 
+ To be able to login you need to change the key permissions and you need to be in the location where the key is download or found 
+ To login into an EC2 instance through ssh security inbound rule port 22 must be open . The common troubleshooting error you will see when its connectivity issues like ports `Operation timed out` 


**Other methods of logging into an EC2 instance without using SSH** 

+ **EC2 instance connect** : 
The ec2 instance must be in the public subnet and has a public ip address and use a user name 
the default username is Ubuntu if you are using an ubuntu server .



Provisiong our first instance 

+ Give good naming conventions based of the software running in the computer  (e.g webservers , jenkns , sonarqube, database ) because it helps to differentiate what workload is running in the servers 

+ Next Select An `Amazon Machine Image ( AMI)`
+ Select Instance Type 
+ Create a Key pair : We can login into our ec2 through two authentication methods ( Keypairs or Passwords , ). When you create a key pair it will download the key to your download folder on your computer . 
+ 


`cases for ami in aws` 

+ we can use it to Launch new instances 
+ we can use AMI for automation and scaling 
+ We can use AMI for version control 
+ We can use to migrate an ec2 instance from one region to another and one aws account to another 
+ We can use AMI for backup and disaster recovery


+ Private AMI 
+ Custom default AMI from AWS 
+ AMI from AWS market place ( Public AMI)




How to login into AN ec2 instance 

+ Locate the instance you just created by click EC2 and click `instances running` 



**KEY PAIRS** 

What are the different ways a user can login into an ec2 instance ? 
a user can be authenticated to login into an ec2 instance using the following methods : 

+ Key Pairs 
+ Password 

The process of logging in an EC2 instance using key pairs and password is called `SSH (Secure Shell )`

+ A key pair consist of a public and a private key .The Public key is stored in AWS .While the private key is stored or kept by the user .Key pairs are primarilly used for SSH Access. 
+ An ec2 instance without a keypair you wont be able to SSH into the server but you can still login to the server using different methods or approach . 
+ You can use a single keypair on several ec2 instances and you can use different keypairs on different ec2 instances 
+ When naming a keypair use a single name , do not put spaces in the naming convention 
+ When you create a keypair for the first time it goes to your downloads 
+ To be able to login you need to change the key permissions and you need to be in the location where the key is downloaded or found and the command is `chmod 400 <name-of-keypair> this can be found under downloads or aws when you click on connect on the ec2 instance and select SSH 
`Note` : When you login into your ec2 instance succesfully for the first time meaning you change the permission of the key and was able login , for your next logins you do not need to run the permission command anymore . 
+ To login into an EC2 instance through ssh security group inbound rules for port 22 must be open . The common troubleshooting error you will see when its connectivity issues will be`Operation timed out` 


**Other methods of logging into an EC2 instance without using SSH** 

+ **EC2 instance connect** : 
The ec2 instance must be in the public subnet and has a public ip address and use a user name 
the default username is Ubuntu if you are using an ubuntu server .
You can also connect to the private subnet using same approach if you have an instance connect endpoint 

+ **Session Manager** : 

with session manager you can connect to your ec2 instance without : 
+ Connect to your instance without SSH keys, a bastion host, or opening any inbound ports.
+ Sessions are secured using an AWS Key Management Service key ( KMS- its used to encrypt data).
+ You can log session commands and details in an Amazon S3 bucket or CloudWatch Logs log groups.
+ You must attached an IAM role in other for your ec2 instance to have permission to use the SSM service in aws the permission you need to attach are (`AmazonEC2RoleforSSM` AND `AmazonSSMManagedInstanceCore` ,).
+ Note you can use a single SSM role to Many EC2 INSTANCES at same time
+ To update your ec2 with the IAM role you have created , click on instance -----> click actions -----> select security ----> click modify IAM Role ---> select the IAM role you create and save . 
+ you can access your ec2 instances both in private and public subnets using this approach 

+ **EC2 SERIAL CONSOLE** 
+ **SSH (SECURE SHELL)**  : you will use keypairs and passwords for authentication & port 22 must be open .
+ **RDP(Remote Desktop Protocol)** : This logs into the server which gives a graphical user interface for your Server 

STEPS TO SETUP YOUR RDP connection 

+ Install update your server `sudo apt update`  
+ install xrdp sererv : ` sudo apt install xrdp`
+ Start the xrdp service and enable it so that when we reboot our system it will be running ` 


You connect to your EC2 instances and aws resources using `CLOUDSHELL` 

+ Cloud shell is an integrated ,browser based shell environment that provides you the ability to run command line (CLI) to manage your aws resources.It allows you run commands , scripts , to interact with your aws services without you installing any software . 
+ Session Manager 
+ EC2 serial console 



**Security Groups :**  regional resources 

+ Security are what we call Virtual Firewalls . This controls the network traffic in and out of your ec2 instance called inbound and outbound traffic.
+ This is security at the instance level 
+ By default when you create an instance it comes with a default security group 
+ You need port 22 inbound rule open on your security group  before user can ssh into  your ec2 instance
+ A security group must be attached to a VPC (virtual private cloud)
+ You can attach a single security group to multiple ec2 instance within the same Region 
+ When you configure a source like 0.0.0.0/0 : it means you are allowing access from anyone and anywhere .This is not recommended and not a good security practise 
+ apart from using the source as ip address cidr like  (`0.0.0.0.0/0 or 123.495.697/32`) you can also use configure the source as a security group id `sgr-08adaec2ddb600c5f`
+ Security groups are stateful in nature If it allows inbound request or traffic into your ec2 instance , it automatically allow outbound request without setting any outbond rules . ***
+ When you create a security by default it denies  all inbound traffic and allow all outbound traffic .


source ip :  Where is the traffic coming from ?
destination ip : where is the traffic going to 

Interview Question 

+ How do you ensure your resources in the cloud are secured 
+ Can you tell me your experience with cloud network security 


**Best Practise when configuring a security group** 

+ Ensure you follow the principle least previllage 
+ Avoid granting access to sources like 0.0.0.0 /0 grant access only to trusted ip address or security groups
+ Have one security group just to manage SSH connections 
+ Monitor security groups regulary (Audit)


**What is the difference between a security group and a NACL (Network Access control list)**

security groups are stateful while nacl is stateless
security group is firewall at ec2 instance level while NACL is firewall at subnet level 

*Why should we set outbound rules if by default security groups are stateful ?* 

+ For restricting outbound traffic to pariticular sources 
+ to prevent Data exfiltration . that is allowing sensitive data to be send out to bad actors . 
+ for compliance purposes , if your company needs more restriction for inbound and outbound traffic at instance level.

**HOW T SSH FROM INSTANCE TO ANOTHER USING SECURITY AS SOURCE*




**EBS VOLUMES** 

EBS - Elastic Block Store it is scalable , high perfomance block storage .This is compared to a hard drive on a physical computer 

When you create a Virtual Machine (EC2) it comes with an EBS volume .The full acronyme for ebs is `Elastic Block Store` 


**Features of an EBS volume** : 

+ *Persistent Storage* : The EBS volume can persist data regardless of the life cycle of the ec2 instance. that is if you shutdown the instance and restart you wont lose data. 
+ *Easy to scale* : you can increase the size of an ebs volume , create new ebs volumes and attach to ec2 instances (Vertical scale and Horizontal scaling)
+ *Highly available & Durable** they can be replicated across different AZ (data center)
+ *You create snapshots* Snaps shots are simply a replica of the current state of your ebs volume which can be used for point-in-time recoveries.You can use snapshots to backup data OR  migrate data .When you take snapshot of your EBS its stored in an S3 bucket.
snapshots are incremental backups , this means only the blocks (data) that was not previously captured in the snaptshot will be captured.
+ Perfomance Options : EBS has different types of options you can select base on the storage perfomance for e.g
(General Purpose SSD, Provisioned IOPS SSD , Throughput Optimized HDD , Cold HDD , Magnetic Standard) 




**VOLUMES :** 
**Different Types of EBS VOLUMES** 

**General Purpose SSD (gp2 ,gp3)** : 
This is aws trying to give you a storage that can use for multi purpose use by balancing it with price and perfomance 
It is good with different varieties of workloads ,and applications and gp3 volumes (storage) offer high performance and lower cost 
than gp2. 
+ Cost effective and has low latency 
+ Its good for system boot volumes , virtual desktops , development and test environments

**Provisioned IOPS SSD (io1 ,io2) :** 
This EBS volume type is designed for I/O -Intensive applications such as Datases because databases need persistent , consistent and higher IOPS. io2 volumes it offers higher durability than io1.

+ IOPS  means `input and output Operations per second` 

**Throughput Optimized HDD (st1**): Its designed to handle frequenly accessed throughput intensive workload . Low cost e.g Data warehousing , log processing 


+ Throughput : in the context of computer networking and storage , This is when data is succesfully transfered from one location to another and its measured in bits per second (bps ), Mbps (mega-bits per second) , gigabits per second. 


**Cold HDD (sc1)** : This storage is designed for less frequently accessed data.
It provides low cost per GB . This is good for scenarios where price is more of a concern than perfomance . 
e.g its good to save backup files or files serves in ec2 . 

**Magnetic Standard** : Its the previous generation of the HDD .This EBS type is not recommended due to its lower perfomance and its high cost when compared to other new ebs storages . 




**Steps to Increase an EBS Volume

**Step 1: Identify the EBS Volume**

Find the EBS Volume ID attached to your instance or locate through your EC2 instance under Storage 
Verify the current size of the volume.


**Step 2: Modify the EBS Volume**

You can modify the volume via AWS Console

Option 1: Modify EBS Volume via AWS Console

Open the EC2 Dashboard.
Navigate to Volumes under the Elastic Block Store section.
Select the EBS Volume you want to resize.
Go under actions and select Modify Volume
Enter the new size (GB) for the volume.
Click Modify, then Confirm.



**Step 3: Extend the Filesystem**
On the commandline run the following commands

`lsblk`
`sudo growpart /dev/xvdf 1`  # Replace xvdf with your disk name
`sudo resize2fs /dev/xvdf1`  # Replace xvdf1 with your partition






#####################
EBS SNAPSHOTS 
#####################

All ec2 instances comes with a root volume (EBS)

*Root Volume* : The root volume is refered to as the primary storage device attached to a virtual machine (ec2 instance)
It contains the operating system ,system files and any other dependencies that requires the instance to function 

*Data Volume* : This is refered to the storage volume primarily used for storing application data , user data , and any other type of non system data 

`What is a Data`a : This is any information that can be stored e.g name can be data , files can be data . Data can be classified ( Sensitive (only authorized users can have access to it ) or non sensitive data (anyone can see or have access to it ). 

What are snapshots : This is a copy or replica of your EBS Volume .

By default when you create an aws account , ec2 ebs encryption is turned off . you will need to manually turn it on .
What happen if you have already created EBS volumes with encryption not turn on . 

What is Encryption ? : This is simply turning plain text into cipher text. This is when you mask or hide sensitive data from non authorized users using what we call encyrption keys 

+ Encryption at Rest : This is when the data is stored in a storage and not moving and its secured using encryption keys
+ Encryption in flight : When the data is moving from one location to another 





**HOW CAN YOU ENCRYPT AN EXISTING EBS VOLUME THAT HAS NOT BEEN ENCRYPTED** 

+ aws offers encryption service through KMS (Key management service ) this service offers encryption at rest 
it has default kms key or you can create a custom kms key .
+ Certificate Manager (offer data encryption in flight ) SSL /TSL certificate 

**STEPS TO ENCRYPT AND UNECRYPTED EBS VOLUME :** 

+ Identify the ebs volume that has not been encrypted 
+ Create a snapshot from the existing ebs volume (if the ebs volume is attached to a running instance)
stop the ec2 before creating a snapshot) . Note you can still create the snapshot while the instance is running. To create the snapshot click on selected ebs volume and click on actions and create a snapshot . 
+ Navigate to snapshots and locate the new snapshot that you have created 
+ Create a volume from the snapshot that you created .go on action and `select create volume from snapshot` when you are creating the new volume you can now encrypt the new volume. 
`Note` : The new ebs volume you are creating make sure its located in the same Availability zone as the ec2 instance where you need to attached the ebs volume to 


Another method of encyption :

+ create a snapshot of the ebs volume
+ create a copy of the snapshot and while creating the copy of the snapshot ,encrypt the copy 
+ Create a volume from the encrypted ebs snapshot and the volume will be encrypted 




**What are they advantages of taking snapshots or the feature in ebs volume**

Snapshots can be used for various purposes : 

+ Data protection and point in time recovery 
+ Data Migration 
+ Compliance and Auditing 
+ System Maintenance : Its good to take snapshots when doing system upgrades so you can easily roll back 
+ Backup Data

Data migration : 

+ we created a snapshot from an existing ebs volume 
+ from the snapshot we created a volume 
+ we attached the volume to a new ec2 instance with the data inside the volume 



# Life CYCLE Manager : 

+ This is feature that helps you to automate the creation of EBS snapshots.
+ You can create a life cyle policy to automatically take snapshots on your behalf at specific time 
+ You can create a custom policy and or a default policy .
+ For custom policies you can create an `EBS SNAPSHOOT POLICY` ,`EBS AMI BACKED CUSTOM POLICY` , `CROSS ACCOUNT COPY EVENT POLICY` 

+ EBS SNAPSHOOT POLICY : This policy creates EBS SNAPSHOTS on your behalf 
+ EBS AMI BACKED CUSTOM POLICY: Its going to create  AMI for your instances on your behalf 
+ CROSS ACCOUNT COPY EVENT POLICY` : It copies EBS or AMI across your aws account to another account 


HOW CAN WE COPY AN AMI FROM ONE AWS ACCOUNT TO ANOTHER (Customer KMS )
EFS ( ELASTIC FILE SYSTEM ) 



**Default Life Cyle Policy** 
+ With this policy you can exclude ebs volumes that you dont want to take snapshots using tags 

**Custom Life Cyle policy `EBS Snapshots Policy`**
+ you have to setup Tags for the resources e.g EBS or instance that you want the policy to target and create snapshots


##########################
Monday July 29 ,2024
##########################

Copy AMI or EBS volume from one AWS region to another 

**HOW CAN WE COPY AN EBS VOLUME or AMI from AWS account to another**  

**AMI* : 
+ Create an Image from your existing EC2 instance (select instance , click on actions , and select create image )
+ On left bar ,click on AMI's and select the image you created ( click actions and select `modify permissions` )
+ Paste the account you want to copy the AMI To . 
+ Note : You can share AMI from one aws account to another if the EBS volume is not encrypted .you can share without any issues but if the EC2 instance that you created the AMI image from has an ebs volume that is encrypted with Default KMS key or a custom KMS key that does not grant permission to the destination account you wont be able to share the AMI . 



What can cause you to migrate AMI from one aws account to another or region to region 

+ Backup Data (High Avaialable)
+ Security reasons : standalone aws account ( production , stage , development )  


**EBS SNAPSHOT RECYCLE BIN**

The EBS Snapshot Recycle Bin is a feature that allows you to protect your EBS snapshots from accidental deletion. When you delete a snapshot, it is moved to the Recycle Bin, where it is retained for a specified period before permanent deletion. This gives you the opportunity to recover the snapshot if it was deleted by mistake. Here's how you can set up and use the EBS Snapshot Recycle Bin:

The snapshots recycle bin can store both ebs snapshots and ami images 

+ You will need to create a retentionr rule 
+ The retention rule is regional . 



SNAPSHOTS ---INCREMENTAL 

BREAK : 9: 25 PM EST 




**EBS SNAPSHOT RECYCLE BIN**

The EBS Snapshot Recycle Bin is a feature that allows you to protect your EBS snapshots from accidental deletion. When you delete a snapshot, it is moved to the Recycle Bin, where it is retained for a specified period before permanent deletion. This gives you the opportunity to recover the snapshot if it was deleted by mistake. Here's how you can set up and use the EBS Snapshot Recycle Bin:

The snapshots recycle bin can store both ebs snapshots and ami images 

+ You will need to create a retention rule 
+ The retention rule is regional . 



Thursday Feb 20 


############################
# FILE SYSTEMS 
############################

**EFS : Elastic File System**

its is a cloud-based file storage service provided by AWS that offers a simple, scalable feature . elastic file system can be use with AWS Cloud services and `on-premises` resources. It's designed to provide scalable file storage that can be mounted on thousands of Amazon EC2 instances and is also accessible from on-premises servers, making it ideal for a wide range of applications and use cases. EFS is built to be highly durable and highly available, with data stored across multiple Availability Zones (AZs).

+ Its Fully Managed Service . ( Infrastructure management hardware , Operating system , networking ,it handles scaling (vertical or horizontal scaling), Patch Management , Backup & Recovery , Monitoring )  

+ Scale performance 
+ Highly Available 
+ Its compactible with Linux based Operating systems . It does not support windows based operating systems 
+ Cost Effective 
+ Secured 
+ You can connect your EFS from aws cloud to on premise using VPN or Direct Connect (transit)
+ You can mount multiple EFS  system on a single ec2 instance 
+ You can create multiple mount point from a single EFS file system 


**USE CASE** : 

+ Web services and content management 
+ backups 
+ Data migration ( migrate data from on premise to cloud and vice versa )
+ Applications 


**EFS (ELASTIC FILE SYSTEM) STORAGE CLASSES** 

It offers multiple storage classes . 

1. **Standard Storage Classes**
+ **EFS Standard** : This storage class is designed for frequently accessed files 
+ **EFS Standard-infrequent access (Standard-IA)** : Its designed for files that are not frequently accessed .
EFS automatically moves files that haven't been accessed according to the lifecycle policy (usually 30 days) to the Standard-IA storage class to reduce costs while still ensuring data is accessible without retrieval delays. This class charges for data retrieval, so it's best suited for infrequently accessed data. 




2. **One Zone Storage Classes**  (data is stored in a single availability zone )
**EFS one Zone** : This storage classes allows you to store your data in a single availability zone and this data stored in this single availability zone will be frequently accessed.

**EFS one zone infrequent Accessed (one Zone IA)**: Data is stored in a single availability zone and the data is not frequently accessed . It offers a lower storage cost when compared EFS one zone and the EFS standard IA . 

+ with on zone storage class you can not transition your files or data to Archive storage 

+ High Available storage ?
+ Cost Optimization 
+ Durable access pattern 
+ Scale (workload unpredictable)



**Lifecycle management for EFS**

Transition into Infrequent Access (IA):
30 day(s) since last access
Transition into Archive:
90 day(s) since last access
Transition into Standard:
None


**SETTING UP EFS ( ELASTIC FILE SYSTEM)**

When creating an EFS you will need to select the performance mode for your efs in regards througput .
Throughput in the context of  computer networking and storage , This is when data is succesfully transfered from one location to another and its measured in bits per second (bps ), Mbps (mega-bits per second) , gigabits per second. )
we will need to select either the `enhanced or bursting mode` .
Within the enhanced mode we have the Elastic or Provisioned 


**Perfomance Settings** :

`Different Throughput modes` 

+ `Enhanced` :    ( Elastic and Provisioned )

+ `Bursting Mode`  : Default throughput mode where performance scales with the size of the file system.Provides baseline throughput with the ability to burst up to higher levels.


**File system policy** 

These are policies you can set to enhance security on your EFS and some of the policies you can set up are : 

+ Prevent root access by default*
+ Enforce read-only access by default*
+ Prevent anonymous access
+ Enforce in-transit encryption for all clients



EFS Security group I.D   :   sg-038bd7ba01c7f3f40


EFS mount point: /mnt/efs/fs1

**Steps to Create EFS** 

+ We created an EFS file system and we should the Throughput Enhance - Elastic 
+ While creating the file system we created a custom security group and attached to the EFS . 
+ We created 2 ec2 instances with the same security groups 
+ We going to edit the the security groups ( EFS security group inbound rules and EC2 instances Inbound rules) to accept Port  NFS PORT : 2049


**Difference Between EFS and EBS volume** 

+ Ebs provides a block level storage vs EFS offers file level storage which allows multiple ec2 instances to access the same file simultaneouly 
+ Scalability : Ebs volumes needs to be scaled manually  but with EFS you can scale manually and automatically . 
+ Attachment and mounting; for ebs volumes you can attach to only one instance at a time . Multi Attach enabled ebs volumes (oi and o2 types) and with this you will only be able to attach in a single availability zone 
while EFS you can mount on multiple ec2 instances at the same and different availability zones. 
+ Backup & recovery. Ebs uses snapshots which can be used for point in time backup of volumes.while EFS you can enable automated backups this is managed by AWS. 
+ Cost and provisioning .EBS requires users to provisioned (that which storage capacity you want) when you do this you have anticipated the storage you will need. On other hand EFS will scale in or out automatically if demand increases ir decreases 



AMAZON FSX


**AMAZON FSX** (`Fully managed third-party file systems optimized for a variety of workloads)`
 Its a file storage service offered by aws and its fully managed service and its to launch and run popular file systems with good performance ,security . It can be used for different use cases .FSx Provides fully managed third party file systems for for native compatibility (match) and features .

 Amazon FSx is a fully managed service that provides file storage optimized for a wide range of workloads. It enables organizations to deploy and manage shared file systems in the cloud, integrating seamlessly with other AWS services. FSx supports multiple file system types tailored to different performance and compatibility needs.


**DIFFERENT FILES SYSTEMS UNDER AMAZON FSx** 

 - FSx for NetApp Ontap
 - FSx for OpenZFS
 - FSx for Windows File Server
 - FSx for Lustre 

 ### FSx for Windows File Server

 - This is a fully managed windows file system share drive 
 - it supports SMB protocol and windows NTFS
 - It supports Microsoft Active Directory Integration , ACL , user quotas
 - it can be mounted on a linux EC2 Instance 
 - you can scale this file system up to 10 s of GB 
 - You connect it to your onpremise network using a VPN or Direct Connect 
 - It is highly available because you can configure to be in different availability zones ( data centers)
 - You can back up the data daily into an s3 bucket 


# USE CASE 

Hosting a business application , if you require a workflow that requires a windows file storage .

## FSx for Lustre 

- The name lustre is taken from the word  linux - cluster (lustre )
- This is a fully managed file system that is optimized for high performance computing , machine learning and media processing workloads. 
it can handle large amount of data processing workloads .
- it can scale up to 100s GBS/S , millions IOPS
- it has different storage options ( SDD , HDD)
- You can intergrate or connect with S3 
- You connect it to your onpremise network using a VPN or Direct Connect 

## FSx for NetApp ONTAP : 

- This service provides a managed NetApp ONTAP file system with full support for the ONTAP data management feature 
- It supports NFS . SMB and ISCSI 
- it is good for enterprize applications , databases , and storage for virtual machines (ec2 instances ) (VM)
- it works with the following operating systems 9 Linux , windows , MacOS , EC2 , EKS ,ECS, Vmcloud on aws 
- It can shrink storage or it can grow the storage automatically 
- Snapshots , replication , low cost , compression and data de-duplication

USE CASE : Disaster recovery, backup , virtualization 


### FSx for OpenZFS

This service provides a managed file system that provides high performance and rich set of data management capabilities 
- it supports snapshots 
- it supports cloning 
- it supports data compression
   all these makes it a use case for software development environments , data analytics application , media processing workflow 
- 1 ,000,000 IOPS with < 0.5ms latency 



**Advance Settings (Details) for OUR EC2 instances**



+ **Domain join directory*: This is when you want your instance to have domain based authenticattion with a centralized authentication service like Active Directory  , SSO .

+ IAM instance profile  : This settings helps you to grant access to your ec2 instance using IAM roles 


What is a Hostname*: This is a label(name) assigned to your device (computer or server) on a network thats helps to identity the device in a human readable format .

+ **Hostname type* : This is a feature in EC2 when you want to automatically assigned a DNS name to your ec2 instance .It has ip name and resource name options . You can have a public and a private hostname

+ *Instance auto-recovery*: It helps to ensure availability and reliability of your instances by automatically recovering from any hardware failures or issues 

+ *Shutdown behavior* : This is an action an EC2 instance will take or receive when it receives a shutdown signal .You can configure your ec2 instance in such away how it will response when its been shut down . e.g stop , the instance will shutdown this is similar to when you turn off your compuer (you can start your instance later , ebs data is still intact , you wont incure any changes) but when you terminate (the instance will shutdown and will be deleted and this action is not reversible)

+ *Stop - Hibernate behavior* When the instance is shutdown it will hibernate ,This is going to save the instance RAM content to the EBS root volume , allowing your instance to resume in the exact location . You might hibernate an ec2 if you need to resume the instance faster . Also when your hibernate the aws will not charge you but you might incur charges for the RAM Content 

+ *Termination protection* :This is when you want to protect your ec2 instance from unintended Termination 

+ *Stop protection*This is when you want to protect your ec2 instance from unintended Stop .

+ *Detailed CloudWatch monitoring*: This will monitor your ec2 instance such as metrics like EBS volume etc . 

+ *Credit specification*: This is a settings that applies to Instances that uses CPU credit for bursting performance.e.g T3 ,T2 , T3A 

+ *Placement group* : This is a way you place your ec2 instances in the cloud to meet various aspects liker perfomance and availability requirements . 

+ **Kernel ID* : Select the kernel ID for the ec2 instance Architecture you prefer.
+ **License configurations* : This is when you have your own license and want to use for your EC2 instance to manage software applications that runs on the ec2 instance . 





**Placement Groups**

A placement group in AWS (Amazon Web Services) is indeed a configuration option that allows you to control how a set of interdependent instances (servers) are physically positioned within the AWS infrastructure. By using placement groups, you can set the deployment of these instances to meet specific requirements of your workload, such as an application, web server, or database, with considerations for aspects like low latency, high resilience, or high availability. 

Placement groups are particularly useful when your workload demands a high level of communication performance between instances, or when you have specific needs regarding the physical proximity(closeness) of your virtual servers (ec2).



1. **Cluster Placement Group** : A cluster placement group is a logical grouping of instances within a single Availability Zone. Instances in this group are placed close together to provide low latency and high throughput networking.

*Use case* : This is good for applications that require high network performance 

*Disadvantages of Cluster Placement Group   

+ Single availability zone 
+ Limited Instance Types. Some instance types may not be supported by the cluster placement groups


2. **Spread Placement Group** : A spread placement group distributes instances across multiple underlying hardware racks within a single Availability Zone. This ensures that instances are placed on separate racks to minimize the risk of simultaneous failure.

*Use Case*: Suitable for applications that need to be highly available and resilient to hardware failures. It is often used for critical applications where you want to ensure that no two instances are on the same hardware rack.

**Limitation*

+ Limitation with the number of instance you can launch in a single AZ  you can have 7 instances per AZ though the number can change depending the Region.
+ Reduced network performance when compared to cluster placement group  .
+ Availability Zone Restrictions: While spreading instances can enhance availability, it also means that your application components might be spread too thinly across AZs, potentially impacting performance due to inter-AZ latency.


3. **Partitioned Placement Group :** :  A partition placement group divides instances into logical partitions, where each partition is placed on distinct hardware to avoid single points of failure. Instances within the same partition share the same hardware but are isolated from instances in other partitions.

USE CASE : Ideal for large distributed and replicated applications that require high fault tolerance, such as Hadoop, Cassandra, or MongoDB clusters.

*Limitations*

+ Complex to manage especially as you scale 
+ Limit on partition.There's a limit to the number of partitions you can have within a placement group, which can vary by region. This limit might restrict how you scale your application.
+ inter partition latency : While instances within the same partition benefit from low-latency connections, communication between instances in different partitions might experience higher latency, as they could be located in different physical areas within a data center




**USER DATA*

Amazon EC2 offers a feature where you can do a pre-configuration of your EC2 INSTANCE at launch time . you can install packages using a bash shell script . This process is know as `Bootstrap` .

User Data in Amazon EC2 (Elastic Compute Cloud) refers to the script or data that is provided by the user when launching an instance. This data is used to perform automatic configurations or run scripts on the instance as it boots up for the first time. User data can include shell scripts, cloud-init directives, or other automation data that applies to your instance to configure software packages, install applications, manage files, or perform other bootstrapping tasks.

example : 

#!/bin/bash 
sudo su - 
apt-get update -y 
apt-get install apache2 -y
systemctl start apache2
systemctl enable apache2
echo "<html><h1>App2: Welcome to TEAM4TECH SOLUTIONS CLASS8 USER DATA DEPLOYMENT</h1></html>" > /var/www/html/index.html


+ Static web application (content does not change user must manually update the content )
+ Dynamic web application (content changes all the time . e.g facebook , amazon )
+ Standalone web application : This is a web application that uses the users Operating system to work without any webserver and it does not need internet to function e.g Microsoft word 


U CAN USE USER DATA FOR MULTPIPLE 

+ Software installations or package installations
+ Application deployments ( static websites )



**LOAD BALANCING**
Under EC2 instances we have a feature called Load Balancing 

Load Balancers 
Target groups 

What is a Load Balancer : ?

This is a device or software that helps to distribute your network or application trafic across multiple devices.The goal is to ensure no single server(ec2 instance) should be overwhelmed with too much traffic , when your server is not overwhelmed with too much traffic this improves your application perfomance and reliability.


## HOW DOES A LOAD BALANCER DETERMINED WHEN AN EC2 INSTANCE (WEB SERVER) HAS LESS LOAD BEFORE IT ROUTES TRAFFIC TO IT . 


A load balancer will monitor your servers : 

1. **Perfomance Metrics** : Cpu usage , Memory , Responsive Time ,these metrics gives the loadbalancer an insight on how much load a server has . 

2. **Round Trip Time** : Some load balancers measure the round trip time ,which means a longer round trip time indicates the servers might be experiecing more workload .

3. **Load Balancing Algorithms** :  Load balancers uses Round-Robin algorithm to distribute request equally regardless of the load. it uses both the health check and perfomance metrics

**There other alorithms** : 

These are features you can set on your application loadbalancer to control it on how it distributes traffic to your backend servers.

+ *Round Robin*: Distributes traffic evenly across all healthy instances in a rotating order.
+ *Least Connections*: Directs traffic to the instance with the fewest active connections.
+ *Least Response Time*: Sends traffic to the instance with the quickest response time.
+ *Weighted Round Robin*: Assigns more traffic to instances with greater capacity or better performance, based on predefined weights.
+ *IP Hash*: Routes requests based on a hash of the client’s IP address, ensuring consistent routing for the same client.


4. **Health Checks** :   They check the server to make sure they are available and functioning well . It uses ping to check if the server is responding as expected . servers will respond with HTTP 200 OK 
*Ping*: Sending a simple network ping to see if the instance is responsive.
*HTTP/HTTPS Request*: Making a request to a specific URL and checking for a healthy response (e.g., HTTP 200 OK).
*TCP Connection*: Attempting to establish a TCP connection to a specific port.

5. **Timeout and Error Rates** : It monitors the request timeout and if the load balancer receives a response with timeout it will determined if there is a an issue with the server .


## DIFFERENT TYPES OF LOAD BALANCERS 

1. Application Load Balancer (Layer 7 )
2. Network Load Balancer (Layer 4 )
3. Classic Load Balancer (does not recommend the use . old version) (supports layer 4 & 7 )
4. Gateway Load Balancer (Layer 3)


**What are Layers in Networking**

Networking has what we call layers  

Layers are categorized in different levels 
The OSI (Open Systems Interconnection) model is a conceptual framework used to understand and implement networking protocols in seven distinct layers. Each layer serves a specific function and communicates with the layers directly above and below it. Here's a brief overview of each layer and their roles:

**Layer 1: Physical Layer**

Function: Deals with the physical connection between devices, including cables, switches, and other hardware.
Key Elements: Electrical signals, cabling, connectors, network interface cards (NICs).
Example: Ethernet cables, fiber optics, physical ports.

**Layer 2: Data Link Layer**

Function: Provides node-to-node data transfer and handles error detection and correction from the physical layer.
Key Elements: MAC addresses, switches, frames.
Example: Ethernet, Wi-Fi (802.11), MAC address.

**Layer 3: Network Layer**

Function: Manages the routing of data packets between devices across different networks.
Key Elements: IP addresses, routers, packets.
Example: IP (Internet Protocol), ICMP (Internet Control Message Protocol).

**Layer 4: Transport Layer**

Function: Ensures reliable data transfer between devices, providing error checking and data flow control.
Key Elements: Ports, TCP/UDP protocols, segments/datagrams.
Example: TCP (Transmission Control Protocol), UDP (User Datagram Protocol).

**Layer 5: Session Layer**

Function: Manages sessions or connections between applications. It establishes, maintains, and terminates connections.
Key Elements: Sessions, dialogues, connections.
Example: NetBIOS, RPC (Remote Procedure Call).

**Layer 6: Presentation Layer**

Function: Translates data between the application layer and the network. Handles data encryption, compression, and translation.
Key Elements: Data formatting, encryption/decryption, compression.
Example: SSL/TLS (Secure Sockets Layer/Transport Layer Security), JPEG, ASCII.

**Layer 7: Application Layer**

Function: Provides end-user services and interfaces directly with applications. It facilitates communication between software applications and lower layers.
Key Elements: Application protocols, interfaces.
Example: HTTP (Hypertext Transfer Protocol), FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), DNS (Domain Name System).

Understanding Layer 7 (Application Layer)
Layer 7 is the top layer in the OSI model and is closest to the end-user. This layer interacts directly with software applications to provide communication functions. It's responsible for:

Protocol Implementation: Ensuring that network services are available to applications (e.g., web browsers, email clients).
Data Formatting: Converting data into a format that applications can understand.
User Interface: Providing network services directly to end-user applications.
In the context of load balancing, a Layer 7 Load Balancer (like AWS Application Load Balancer) can make routing decisions based on:


**DIFFERENT TYPES OF LOAD BALANCERS** 

1. `Application Load Balancer ( uses layer 7)`

+ This is good  loadbalancer to route traffic to your web applications 
+ Application load balancers are designed for http (hypertech transfer protocol) and https (hypertech transfer protocol secured) traffic 
+ It operates under layer 7 ( Operates at the OSI (Open system interconnection) Model Layer 7 )
+ Application loadbalancer supports features like SSL/TSL terminations ,websockets and HTTP/2 
+ Application loadbalancers are good for modern application archectures (micro service , monolithic applications)
+ Applications loadbalancers can be internet facing or internal facing . 
+ Load Balancers comes with security groups to enhance security 
+ Load balancers will reach instances through what we call a Target group . A target group  helps a load balancer to target specific resources base on your configurations .



**Target Groups** : 

This is a logical grouping of your endpoint (target) which can be an ec2 instances (servers) where you want your load balancer to distribute traffic to . They can route traffic to one or more registered targets

**Different Target Types**

+ Instances 
+ Ip addresses
+ Lambda functions 
+ Application Loadbalancer 

Creating our first load balancer 


+ 3 ec2 instances and each of them we will deploy a simple static web application using apache 
The reason for creating this instances is because we want to demonstrate how our load will be routed when we create a load balancer 

+ Create a Target group (for unsecured traffic select http port 80 and for secure port 443)


9:05 pm est -  9:16 pm est 


http : This is is unsecured traffic and the port is 80 
https : This is secured traffic ( data in transit or flight is encrypted) port 443 



In aws we have a service called AWS Certificate Manager ( ACM ) and we will use this to encrypt our load balancer on port 443 which is https .

Group 

Names 

Steph  :  Canada 
Mr Divine : Canada 
Scotlinda : Canada 
Veron : USA 
Frank : USA
Dezii : Canada
Fabrice : Canada
Yvonne : USA (Maryland)
Nash : USA
Ronald : CANADA


Group A 

+ Steph      -> Teamlead 
+ Mr Divine  -> Assistant Lead 
+ Dezii 
+ Nash 
+ Fabrice
+ Maurice 


$90,000 devops 

120,000 - 130.000 

2900 x 8

140,000
130,000
120.000

Group B 

+ Scotlinda -> Assistant lead 
+ Veron 
+ Ronald 
+ Frank -> Teamlead 
+ Yvonne
+ Srijana




brew install git 

git clone https://github.com/Team4techsolutions-Class8/Notes.git

make sure you have git installed 




2. ## Network Load Balancer (Layer 4 )

+ Network load balancers are designed for TCP ,UDP and TLS traffic 
TCP: Transmission Control Protocol (IP network)
UDP: User Datagram Protocol 
TSL: Transport Layer Security 

+ Network load balancer operates at layer 4 which offers high perfomance & low latency 
+ Network load balancers often provide a static IP address or a set of IP addresses that remain constant, which can be beneficial for applications that require predictable IP addresses. 
+ Network load balancers offers high perfomance because it operates at a lower layer and handles fewer data processing tasks compared to application-layer load balancers, NLBs typically offer lower latency and higher throughput.
+ Network Loadbalancers performs health checks on targets to ensure that traffic is only sent to healthy instances. These health checks are typically based on TCP connectivity.
+ Network loadbalancers are good for Streaming applications , VOIP ,Video Games 


3. **Gateway Load Balancer (Layer 3)**
+ Its designed to work at layer 3 which is the network layer OSI model
+ It has a single entry point .Provides a single entry point for inbound traffic, simplifying network architecture and reducing the number of routing configurations.
+ It also supports healths check for the virtual instances to ensure traffic is sent to healthy instances 
+ Your target has a deployed Virtual Appliances ( EC2 Or a Third party service )
e.g Firewalls , Intrustion Detection , Prevention systems , Deep Packet inspections 




**Sticky Sessions (Session Affinity)** : 

This is a feature in load balancing where a load balancer ensures that a client or customer is consistently being routed to the same backend server he accessed for a specific period of time or during his or her session on the website 

+ This works with Classic , Application and Network Loadbalancers
+ This feature uses cookie and cookie for each session has an expiration date you can control
+ This is good for stateful applications (stateful applications are applications that uses a database) and this helps users not to lose session data . 
+ Sticky sessions on your load balancers can pose a Limitation for load balancing .we distribute load across multiple backend servers to avoid loading one server too much but with the sticky session configured this can kill the purpose of load balancing . 

**How to setup sticky session**

+ Click on Target Groups and under target groups select the target you want 
+ Under attributes click on edit attributes 
+ Choose the stickeness type (`Load balancer generated cookie or Application-based cookie` )



**How Sticky Sessions Work**

+ *Session Identification*: The load balancer identifies and tracks client sessions, usually by using a session cookie, a URL parameter, or another method.

+ *Consistent Routing*: Once a client is assigned to a backend server (ec2 instance), subsequent requests from the same client are routed to the same server for the duration of the session our example is 1 mins 

+ *Session Data*: This is particularly useful for applications that store session data locally on the server. Without sticky sessions, the client’s requests might be distributed across different servers, which could lead to inconsistent experiences if session data is not shared or synchronized.


**Common Methods for Sticky Sessions**

+ *Cookie-Based Sticky Sessions*: The load balancer inserts a special cookie into the client’s browser. This cookie identifies the server to which the client is assigned. On subsequent requests, the client sends the cookie back to the load balancer, which uses it to route traffic to the same server.


+ *IP Hashing*: The load balancer uses the client’s IP address to determine which server to route the request to. This method is simpler but less flexible and can lead to issues if clients share IP addresses or change IPs frequently.

+ *URL Parameter*: A unique identifier is included in the URL of each request, and the load balancer uses this identifier to route the request to the appropriate server.

**WHY SHOULD WE USE STICKY SESSIONS IN LOADBALANCING ?**  

1. *Stateful Applications* (An application with a database): Ensures that clients interact with the same server throughout their session, which is essential for applications that maintain state or session-specific information on the server.

2. **Improved Performance* : Reduces the overhead of reloading session data from a shared store or database, which can improve application performance and responsiveness.

3. **Consistency*: Provides a consistent user experience by ensuring that all requests within a session are handled by the same server.




**CROSS ZONE LOAD BALANCING** 

This is when your load balancer distributes traffic to your application across multiple AZ(Aavailability zones) within an AWS region

+ By default application load balancers will route traffic across multiple availability zones with no charge . 
+ By Default network loadbalancer comes with cross zone load balancing disabled but you can enable it and you will be charge data transfer moving across different AZ's 
+ Classic load balancers by default cross zone load balancing is disabled , if you enable it you wont be charge . 

##############################################
**AWS Load Balancers ROUTING POLICY OR RULES** 
##################################################

A routing policy is a feature you can set that will determined how incoming request (traffic) are distributed among your endpoints (targets e.g EC2 INSTANCES). In aws there are several routing policies you can configured for both application , network and classic load balancers 

Application Load Balancers routing policies  : 

+ Path-based routing 
+ Host-based routing 
+ Query string or http header 
+ http request method 
+ Sourceip 





1. **Path Based routing** : 

Path-based routing allows you to direct traffic to different target groups based on the specific URL path. This is particularly useful in microservices architectures where different services are exposed under different paths. For example:
Requests to /api might be routed to a target group running your API services.
Requests to /admin might be routed to a target group handling administrative functions.

**How to setup**

Setup:

AWS Application Load Balancer (ALB):
Navigate to ALB in the AWS Console:

+ Go to the AWS Management Console and open the EC2 service.
+ Under "Load Balancing," select "Load Balancers."
+ Choose your Application Load Balancer from the list.
+ tO Configure the Listener:

Select the "Listeners" tab and click on "View/Edit rules" for the listener you want to configure (usually the HTTP or HTTPS listener).
Add a Rule for Path-based Routing:

+ Click on the "+" button to add a new rule.
+ Choose "Add condition" and select "Path" from the dropdown menu.
+ Enter the path pattern you want to match (e.g., /api/* for API requests or /admin/* for admin requests).
+ Add another condition if needed (e.g., combining path with HTTP methods).

Under the "Then" section, choose the action "Forward to" and select the appropriate target group for the matched path.
Repeat the process for each path you want to route to different target groups.
Save the Rules:

Save the configuration and ensure the listener rules are in the correct order since ALB processes rules in a top-down manner.




**Host Based Routing** (`Host header routing rule` )

Host-based routing directs traffic based on the hostname specified in the HTTP request. This is useful when hosting multiple services or applications on the same load balancer but under different subdomains or hostnames. For example:
app.example.com might route to your main application.admin.example.com might route to your admin panel.




**QUERY STRINGS** :  This is a load balancer routing rule you can set based on query parameters . e.g you can set a query string like 
categories=men .   http:shop.team4techsolutions.com/?categories=men 

Question ?
What are the priority numbers use for ? and which is of them is the highest priority ?

+ You can configure different routing rules with different priorities .a single a load balancer can have many routing rules as possible.
Now if the load balancer is receiving so much traffic which of the traffic should it route to the backend server first . 

Developers are going design applications using different architetures (Condo + Building , House (town house , detach houses)

Monoliqthe architecture . micro service / backend ---clothes 

                                                     asychronous communication 
full software :   webpage has all types (women , men , kids )--->   api --->   shopping cart (software)


**HTTP Header** 

Routing based on HTTP request headers is useful when you need to direct traffic based on specific header values. For instance, you might want to route traffic differently based on the user's location, device type, or other metadata passed in the headers.

**How to Set Up**

+ Go to your Application Load Balancer settings.
+ Under "Listeners," edit the rules for your listener.
+ Add a condition using "HTTP header."
+ Specify the header name (e.g., User-Agent) and the value you want to match (e.g., mobile).
+ Assign the request to the appropriate target group based on the header condition.
+ Save your changes.

**Testing:**

+ To test install curl  (Mac use `brew install curl`) if you dont have brew then you can install brew using the command `/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"`

+ Use a tool like Postman or cURL to send an HTTP request with the specific header.
+ Verify that the request is routed to the correct target group.
+ run the following command but make sure to change the "agent: agent" to your headers `curl -H "agent: agent" myapploadbalancer-114619576.ca-central-1.elb.amazonaws.com`  





**HEALTH CHECKS IN LOAD BALANCERS** 



Path-based routing 

http request method 
http header 

9:25 pm est 


*.team4techsolutionsinc.com

app.team4techsolutionsinc.com
app4.team4techsolutionsinc.com
web.team4techsolutionsinc.com





**HEALTH CHECKS IN LOAD BALANCERS** 

This is when your load balancer uses the ping mechanism to check your backend servers (target groups) to make sure they are working properly before it routes traffic to it 


**Understanding Load balancers health checks**

1. **Health Check Configurations** 

+ This are configured at the target group level within the load balancer
+ You can specify the protocol type e.g HTTP , HTTPS , TCP 


2. Health Check Path : 

This is the specific endpoint path or url that your load balancer pings to determined weather the target (ec2 instance) is heathy and is able to handle or receive incoming request . 
/ : this is the default path and it uses the root (/) to test but you can set your own custom path e.g 
/health /healthcheck 

3. Health Check port : by default load balancer carries out its health check using the target port configured for your application . you can change or overide this by using your own custom port 

Target : 80 application port 
application webserver : 80 
jenkins listen : 8080 
sonarqube listen ; 9000
nexus listen : 8081 

4. UnHealthy threshold: This is the number of times your load balancer is going to ping your backend servers (targets) before it determines or mark the target as unhealthy . 

5. Timeout : this is the period of time your load balancer will wait for a response before it marks the target as unhealthy 

6. Interval



**Trust store :**  

A trust store is a repository that contains trusted certificates, usually public certificates of Certificate Authorities (CAs), that are used to verify the authenticity of the certificates presented by remote servers or clients.

*Purpose / Use case* : The trust store ensures that only certificates signed by trusted CAs(Certificate Authority) are accepted, thereby establishing a secure connection between the client and the server.




# AUTO SCALING GROUPS 
 
This is a feature in aws where you can automatically adjust the number of compute resources such as virtual machines or ec2 instances based on demand .This is to help ensure that there is high availabilty of your application 
+ You can increase & decrease based on demand. when you are increasing we term it scaling out and when you decreasing the demand we term it scaling in . 


**Horizontal Scaling* : This is when you add more resources for e.g more ec2 instances if demand is high 
**Vertifical scaling* : This is when you increase existing resource capacity to accomodate the increase in demand . e.g EBS volume increase from 30 to 50MB ,instance type change etc 


Auto scaling groups implement what we call Horizontal Scaling 



**Other Services in AWS that have auto-scaling capabilities** 

- EC2 : Elastic Compute  ( it adjust the number of ec2 instances )
- ECS : Elastic Container service  ( supports auto scaling at task level)
- EKS : Elastick Kubernetes Service : (Horizonal Pod and Cluster Autoscaling )
- AWS Lambda : it will auto-scale the number of lambda function executions in response incoming event rate 
- Dynamodb Database ( automatically adjust read and write capacity )
- RDS ( Relational Database Service) : (Utilizs autos scaling read replicas for Aurora DB cluster)

our focus is on EC2 INSTANCE Auto scaling 



**AUTO SCALING GROUP (EC2-INSTANCE )**

This is a logical grouping of your ec2-instances managed by aws to be to scale in and out when there is demand. In your auto scaling group you can setup Minimum , maximum and desired number of instances in the group . 
With this setup you want to make sure the number of instances running can handle the load ,so the auto scaling group will scale up and down to handle incoming request and traffic 



**Benefits of Auto-Scaling** 

+ You can automatically scale without any manual intervention which reduces operational overhead . 
+ It helps in reducing cost because without an auto-scaling you will provisioned the number of serves ahead of time and you can over provisioned which will cost you more money but with auto scaling you only pay for what you need .
+ It makes your application highly available. Since your application servers can scale horizontally it means your application will respond to incoming traffic or request at all time without any errors . 
+ It enhances fault tolerance or automatic recovery .Auto scaling detects when an ec2 instance receiving traffic is unhealthy and will automatically replace the instance by creating another instance 
+ Auto scaling has flexibility and adaptability because it has different types of scaling policies for e.g predictive , dynamic and scheduled) which allow you to setup auto scaling for your application based on your specific needs . 






`To create an Auto scaling group you need a Launch Template` . 

`What is a Launch Template ?` 

**Launch Template** This is a resource in aws that `contains configuration information` needed to launch an EC2 instance. It enables you to define and manage the necessary settings for launching your instance in a more flexible manner . 

**Difference between Launch Template and AMI ?** 

| **Aspect**             | **Launch Template**                                               | **Amazon Machine Image (AMI)**                                    |
|------------------------|-------------------------------------------------------------------|-------------------------------------------------------------------|
| **Purpose**            | A blueprint that defines configuration settings needed to launch an EC2 instance, including AMI ID, instance type, key pair, security groups, and more. | A pre-configured template that includes the operating system, application software, and other necessary configurations required to launch an EC2 instance. |
| **Scope**              | Covers a wide range of instance configuration settings including which AMI to use, instance type, user data, and security settings. | Specifically defines the software configuration (OS, installed applications, etc.) and the storage settings for the root volume of the instance. |
| **Versioning**         | Supports versioning, allowing multiple versions of a Launch Template to be created and managed. | Does not support versioning. Each AMI is unique, and a new AMI must be created if changes are needed. |
| **Customization**      | Can be customized to override certain settings at launch time, such as instance type or AMI, without creating a new template. | AMIs are immutable; you need to create a new AMI if you want to change the software configuration or update the OS. |
| **Integration**        | Used by services like Auto Scaling, Spot Instances, and EC2 Fleet to ensure consistent and flexible instance launches. | Used within Launch Templates, EC2 instance launches, Auto Scaling configurations, etc., as the base image for the instance's operating system and applications. |
| **Content**            | Contains a reference to an AMI, along with other settings like instance type, security groups, IAM roles, user data, etc. | Contains the complete machine image, including the operating system, installed applications, and all necessary configurations to run an instance. |
| **Flexibility**        | Allows for greater flexibility and reusability. You can change instance-specific settings without needing to create a new template. | Limited to defining the software stack. Flexibility comes from creating new AMIs with the desired software or configuration. |


**Advantages of a Launch Template**

+ When you use a launch template to create ec2 instances this means there consistency with configurations across different env or instances .
+ This simplifies the creation of new ec2 instances . Eliminates manual configurations
+ it supports version control which means you can easily roll back to a version if there was any issues with your new version 
+ You can integrate launch templates with auto scaling to create new instances faster 
+ Reusable because you can create multiple instances using the same launch template and with the same settings. 
+ Also in cost optimization.Your launch template allows you define the instance purchasing options e.g on demand , spot instance . you can also specify the instance type .


`We also need an AMI when creating an Auto Scaling Group` because when creating a launch template we must specify which AMI
it should use to launch the instance. 

AMI 

+ Dont specify the subnet when creating a template for an auto scaling group template this is because when you are creating your auto scaling group
you will need to specify the availability zone and subnets at the time of creation.
instance is in a public subnet : aws recognizes template based on the subnet 



**HOW DOES AUTO SCALING WORK** 

+ Auto scaling monitors your instances. It continously monitor metrics from your ec2 instances or services.When this metric cross predefined thresholds . it triggers the scaling actions e.g cpu is over 80% it should create new instances . 

+ Scaling Actions : Scale up or down . When it scales up that means there is increase in demand , new instances are launched and traffic is routed to that instance and when scales down this means demand has decreased and it will go back to your desired state(number of instances you want at all time). 

+ Maintained desired capacity: Auto Scaling ensures that the number of running instances matches the desired capacity, even if some instances fail or become unhealthy. This capacity is defined in the Auto Scaling Group configuration

+ Health Management. Auto Scaling performs health checks on the instances in the Auto Scaling Group. If an instance is found to be unhealthy, it is terminated and replaced with a new one to maintain the desired capacity.



**STEPS TO CREATE AN AUTO-SCALING GROUP**





**AUTO SCALING POLICIES**

Metrics : A metric is any quantitaive measure used to monitor and assess the performance , health and utilization resources .Metrics allows you to have insights into your infrastructure or application , allowing you to make informed decisions and take appropriate actions 

Auto scaling policies : These are policies you can configured for your auto scaling group to determined when it needs to scale
or not 

1. **Dynamic scaling policies** : Dynamic scaling policies uses real time metrics to scale in aws and it gets the metrics from cloudwatch 
+ `Target Tracking scaling` : with this you will choose metric and set the target value so that your auto scaling will scale was the metric threshold is met (Average CPU ,average network in , average network out , Application loadbalancer request count per target)
+ `Step Scaling` : Adds or removes instances based on the size of the breach. You can define multiple steps with different scaling adjustments.
+ `Simple scaling` : Adds or removes a fixed number of instances based on a single scaling action.e.g cpu more than 80% you can choose how many instances it should add . 

2. **Predictive scaling policies** : This allows you to auto scale based on anticipated changes in demand. It uses Machine learning algorithms to forecaset demand and automatically adjust the compute resources in antipication increase in demand. It uses Data Analysis ,Historical data , Forecasting , it adjust the resources and continue to learn about your infrastructure or application behaviours.

+ Predictive scaling policies are not meant to be used as standalone policies. AWS recommend that you use predictive scaling together with dynamic scaling policies.

3. **Scheduled actions** : Scheduled Actions in AWS Auto Scaling allow you to automatically adjust the number of instances in your Auto Scaling group based on a defined schedule. This is particularly useful for predictable changes in demand that occur at regular intervals, such as daily or weekly patterns


**Auto Scaling Cool Down Period**

This is a concept or feature in Auto scaling that helps to delay or prevent your auto scaling group from launching or terminanting additional instances before the previous ones have completed initilization and stabilization . This is very important because it avoids scenarious where your auto scaling actions do not overlap and cause resource management issues or unintended scaling behaviours . 


Network traffic load can be inconsistent ( 50 access application spike 60% auto scaling an ec2 10 drop 40% )



If you were asked in an interview 

How have you helped in cost optimization in the cloud in your current role 

+ Auto scaling groups 
+ EBS volume resizing 
+ Instance Type 
+ Purchasing option 



RECAP 
1. IAM - Identity and access management 
+ IAM Roles
+ IAM policies 
+ User Management 
+ User Groups 
+ IAM analyzers 
+ Identity providers 
+ Credential Reports 
+ MFA 
+ Password Policies 

2. EC2 

+ AMI 
+ Purchasing Options 
+ Instance Types 
+ How to provisioned or instantiate and ec2 and how to login using (SSH , Session Manager , Instance Connect, Serial console )
+ EBS (ebs snapshots , lifecycle manager , encryption of ebs , scaling an ebs volume -vertical scaling , Migrate data using EBS and how to recover from disaster recovery, storage types , Recycle bin)
+ EFS 
+ Security Groups 
+ Key pairs 
+ Placement Groups 
+ User data 
+ ec2 adanced settings 
+ Load balancers ( Target groups , routing rules ,SSL/TLS )
+ Auto-scaling groups ( AMI , Launch templates )



# ROUTE 53 

What is Route 53 

Route 53 is an AWS service that you use  to manage your DNS (Domain Name System) and you can use it for `DNS management , Traffic management ,domain registration , health checks and DNS security` . 


**Features of Route 53** 

1. `DNS management` : This allows you to perform DNS queries globally and manage domains
2. `Traffic management` : It helps to set routing policies to route traffic to different endpoints based on different factors like endpoint health , geographical location , geo location , latency etc 
3. `domain registration` : Register and manage a domains 
4. `Health checks`: You can monitor the health checks of your endpoints and if its not healthy you can fail over to a healthy endpoint 
5. `DNS security`: You can implement DNS based security policies such domain name system security extensions . 


**Domain Names** : Human readable addresses used to identify and access resources over the internet e.g team4techsolutions.com ----> 102.304.5069
+ In route 53 we can register a new domain 
+ In route 53 we can transfer an existing domain name from another domain registrar like godaddy , namecheap etc 
+ In route 53 when you register your domain and its approved by default you will have a public hosted zones where you can now manage your DNS names .You can then create a Private hosted zone to manage DNS names for private us that is within your VPC 


In a route 53 you can register your own domain e.g www.team4techsolutions.com . so AWS is considered a domain registrar because in route service  we can register domains . 

`What is a Domain Registrar ?` 

**Domain registrar** : is an organisation or company (e.g AWS , Godaddy , Namecheap) that is authorized to manage the registration of domain names.They act as an intermediary (middle man ) between the business ownwer or individual who wants to secure a specific domain and the domain name registry which maintains the database of domain names and their associated information. 


`Lets Have a Closer Look of an Actual Domain`

www. team4techsolutions. com
       SLD               TLD 


**Domain names have different classifications or Hierachy** 

. means Root Domain 

1. **Top-Level-Domains** (TLDs)  (General or generic ,country code ,sponspored Top Level Domians )

We have different types of Top Level Domains.But when we talk about top level domain we are referring to the part of the domain name that ends with dot (.) e.g .com , .org .ca 

+ **Generic or General Top Level Domains** (gTLDs) : These are the most common top level domains and they include 
(.com .org .net .info .biz ) these types of domains can be generally and they are not tied to any specific geographical region.

+ **Country Code Top Level Domains** (ccTLDs) :  These are top level domains that represents a specific country or territories and consist of two letters e.g (.ca .de .cm .jp .eu .uk ) team4techsolutions.ca ,team4techsolutions.uk 

+ **Sponsored Top Level Domains**(sTLDs): These are top level domains sponsored by specific organisations or communities and they often have specific eligibility requirements.e.g (.edu , .gov , .jobs .mil .corp)    universityoftoronto.edu


2. **Second Level Domains** (SLD) : This is the part of the domain name that is located directly to the left of the top level domain. In other words the SLD is directly below the top level domain and is often used to identify the organization or individual entity e.g team4techsolutions.com is a domain name but team4techsolutions is the second level domain .

3. **Sub Domains** :Anything left after the second level domain is the subdomain . In other words These are the extension of a domain name and its used to organized and categorize different sections or services within a domain e.g jenkins.team4techsolutionsinc.com

jenkins.   team4techsolutionsinc.com
subdomain   SLD                  TLD

www.      team4techsolutionsinc. com 
subdomain   SLD                  TLD

www.team4techsolutionsinc.com   -----> Fully Qualified Domain 

**Summary** 



+ Root Domain: .
+ Top-Level Domain: com
+ Second-Level Domain: team4techsolutions
+ Subdomains: www and jenkins or any other name before the SLD 




**DNS Resolutions or DNS lookup** 

`Interview Question` 

What happens when i type `google.com or team4techsolutions.com` 


When you type team4techsolutions.com a series of steps happens involving Domain Name System (DNS) (Route53) occurs to resolve the domain name into an ip address so that your browser can load the corresponding website 

when you type team4techsolutions.com on your browser this is what we call `DNS Query`

1. Local Cache Check : 

+ Browser cache : what happens is , your browser first checks its local cache to see if the ip address for team4techsolutions.com is there from any previous visit and if its not there it goes to check your 

+ Operating system cache it checks your computer (OS) cache now if it does not have any record of this cache (IP)

2. It starts a DNS (Domain Name System ) Query (Request)  Initiation 

+ Internet Service Provider Domain Name Server ( rogers, bell etc )


3. Starts DNS resolver process by checking the followings 

+ It Queries (checks) ROOT NAME SERVER  . The root servers knows where to find the names servers for the Top Level Domains e.g .com
+The root name server then forwards the query to the TLD NAME SERVER which this server now knows the the AUTHORITATIVE NAME SERVER and it then forwards the query to the 

+ AUTHORITATIVE NAME SERVER  e.g team4techsolutions.com .This sever holds the DNS records for team4techsolutions and it then reponds with the ip address associated with the domain . usually the authoritative servers are with your domain name registrar e.g AWSroute 53 

4. It then reponds back to the customer query or dns request , the resolver receives the up address from the authoritatibe name server and then sends it back to your computer , your computer yjem caches the ip address for team4techsolutions so that in the future it can resolve faster. 


Team4techsolutions.com
app.team4techsolutions -------> 102.304.6070



**ICAAN : Internet Corporation for Assigned Names and Numbers :**

ICANN is a nonprofit organization responsible for coordinating the global domain name system (DNS) and IP address allocation. It oversees the management of top-level domains (TLDs) and ensures the stable and secure operation of the DNS.Head Quarter is in California USA , Los Angeles .website icann.org 

`Functions` 

+ Acredits domains registrar 
+ It manages the root DNS zone (.)
+ They coordinate the allocation of ip address spaces
+ Oversees domain name policy development 



**IANA : Internet Assigned Numbers Authority** : This is a branch or function within ICANN responsible global coordination of the DNS root , ip addressing and other protocol resources  and it administers Top level domains including country code TLD , generic top level domain 




**DNS RECORDS IN ROUTE 53** 

When you create a public hosted zone on route53 it automatically creates 2 important record types which are
`NS (Name server) record  & SOA (start of Authority ) record.`

1. **NS RECORDS (NAME SERVER RECORDS)*
What is a NS record ? 
+ NS records tell the world which name servers are responsible for managing your DNS records of your domain 
+ Name servers are specialized to answer DNS queries , like "What is the IP address of google.com ? " 

example of NS record type when you create a public hosted zone 

ns-1641.awsdns-13.co.uk.
ns-94.awsdns-11.com.
ns-1137.awsdns-14.org.
ns-965.awsdns-56.net.



2. **SOA (START OF AUTHORITY RECORD)*

What is an SOA record ? 
+ It contains important information about the domain and how DNS changes for the domain are managed 
+ It identifies the primary DNS server for the domain and includes administrative info like when the DNS was last updated and how dns records should check for updates 

3. **A RECORD (ADDRESS RECORD)* 
+ A records routes traffic to IPV4 (123.456.4568) and some AWS resources (Load balancer , cloudfront distribution)  example of an ipv4 : 123.456.4568
+ Use case : good when you want to point your domain or sub-domain to specific server e.g EC2 that uses IPV4 address typically for websites 

4. *AAAA RECORD* 
+ AAAA RECORD routes traffic to ipV6 (2607:fea8:a5de:1200:90f6:8fa6:ee81:aa7) and some AWS resources (alias to application loadbalancer , alias app runner , alias to cloud distribution)

+ USE CASE : similar with ipv4 but for websites or services that uses IPV6 instead of IPV4 

5. *CNAME RECORD (Canonical Name Record)*
+ Routes traffic to another domain name or aws resources (alaising) 
+ USE CASE : it can be used when you want multiple domain names to point to a single service , or domain e.g www.app.team4techsolutions.com can point to team4techsolutions.com 


6. MX RECORD (MAIL EXCHANGE RECORD): 
+ It specifies mail servers responsible for receiving emails on behalf of a domain 
+ USE CASE : It can be used when you want to configure email delivery for a domain for instance you set a configuration to direct email traffic for team4techsolutions.com to a mail server. 


6. TXT RECORD 
+ Use to verify email senders and other values 
+ USE CASE : Its used to verify domain ownership , its also used for setting SPF (Sender policy framwork)
for emails or it stores other validation or configuration data . 

#######################################################


7. PTR RECORD ( POINTER RECORD) : 
+ It maps an ip address to a domain name  (reverse dns lookup)
+ USE CASE : Its often used to verify the authenticity of servers especially for emails 

8. SRV RECORD (SERVICE LOCATOR)
+ It defines the location (hostname and port) of a specific service within a domain
+ USE CASE: Commonly used for services like SIP (Session initiation protocol) and microsoft services that requires port to connect 

9. SPF RECORD : (Sender PolicY Framework )
+ Its a type of TXT RECORD used to specify which mail servers are authorized or allowed to send email on behalf of the domain. SPF records help prevent email spoofing by verifying that the sender's IP matches the domain's authorized mail servers.

10. NAPTR RECORD (Naming Authority Pointer Record ): 

+ A NAPTR record is used for regular expression-based rewriting of domain names, enabling dynamic redirection of services. It is often used in combination with SRV records and helps translate URNs (Uniform Resource Names) into URLs or IP addresses.
+ use case : VoIP and ENUM (Telephone Number Mapping) often use NAPTR records to dynamically resolve phone numbers or services into SIP addresses.

11. CAA ( Certificates Authority Authorization)

+ A CAA record is used to specify which Certificate Authorities (CAs) are allowed to issue SSL/TLS certificates for your domain. This helps protect against unauthorized issuance of certificates.

USE CASE : The CAA record helps prevent Bad Certificate Authorities from issuing unauthorized certificates for your domain, increasing security.

12. DS RECORD (Delegation Signer Record)
+ A DS record is used in DNSSEC (DNS Security Extensions) to secure DNS queries. The DS record holds a hash of the DNSKEY (DNS Public Key) for a delegated zone. It helps verify that the DNS records haven’t been tampered with.

USE CASE : DS records are used in conjunction with DNSSEC to provide cryptographic verification of DNS data, ensuring that the DNS records are genuine and have not been altered by an attacker.


Questions ?

**TIME TO LIVE (TTL)** 

**TTL in Route53**  : Time To Live refers to the amount of time that a DNS resolver (like your browser dns cache or your ISP DNS server) should cache the DNS record before it checks back with the Authoritative DNS server (Route53) for an updated version of the record .In other words TTL defines how long a DNS response is valid 

+ Setting a High Time to Live (TTL) for example 24 hours means less traffic to your route 53 but this can lead to outdated information especially if the website or resource is dynamic 

+ Setting a low time to live (TTL) will lead to more traffic to your aws route53 which will show updated information to clients but can be costly 


what is the difference between time to live (TTL ) and Sticky session 

+ TTL is how often users will query dns (route 53) to get new information whereas sticky session is how long the dns will resolve with a specific backend server (ec2) application 





**ROUTING POLICIES IN ROUTE 53**

Note this is not the type of load balancer routing . (routing rules for load balancers)

+ Route 53 offers this feature to help you to manage and control how DNS queries (request) are answered (routed) based on your specific needs.These policies determined how route53  its going to respond when a user request your domain resources (e.g webservers , database , or applications).The different routing policies are going to help us to manage traffic distribution , failover machanism , how it can improve latency ) 

www.team4techsolutions.com ------> resource (web server (hosting website) , database , application , api)



**TYPES OF ROUTING POLICIES IN ROUTE 53** 
To demo routing polcies we have created 3 ec2 instances in 3 different regions , US-EAST-1 , CA-CENTRAL-1 AND EU-WEST-2 using the following user data by changing the regions 

#!/bin/bash
sudo su - 
apt-get update -y
apt-get install apache2 -y
systemctl enable apache2
systemctl start apache2
echo "<html><h1>WELCOME TO TEAM4TECH SOLUTIONS INC FROM CA-CENTRAL-1 THIS IS CLASS-6 </h1></html>" > /var/www/html/index.html


eu-west-2 : 18.171.171.56
us-east-1 : 52.2.199.90
ca-central-1 : 35.183.21.159



1. **Simple routing Policy :** : 

+ This routes dns queries or traffic to a single resource such as webservers , amazon s3 , cloudfront distribution. 
+ Its a default routing policy when you are creating record sets in route 53 
+ we can specify multiple values (ip addresses) in the same record (a random value is going to be choosen by the client
+ You can also use an alais to route traffic to different endpoints e.g loadbalancer and use simple routing 
+ This routing policy can not be associated with route 53 health check 

USE CASE : You can use it when you want dns queries to be routed to a single resource or EC2 instance . 


2. **Weighted Routing**
+ This policy will send all dns queries or request or route the traffic based on assigned weights . You can distribute traffic across multiple resources in proportion of  weight 
+ It controls the percentage of traffic that goes to a single or a specific resource e.g ec2 instance 
+ Traffic % : Weight of a specific record 
              ---------------------------
              Sum of the weight of all records 

+ The weight does not really need to sum up to %100
+ The Record name (DNS name) must be same and the record type must be weighted 
+ Weighted Routing Policy can be associated with Health Checks 
+ When you assigned a weight of 0 (zero) route 53 stops sending any traffic to the resource ( ec2)
+ if you set the weight on all  records to 0 (zero) then all records will return equally 

USE CASE : This is useful for testing new versions of your application (A/B testing) or when you want to gradually roll out changes .
e.g You have release a new version of your application and you want to send 70 % of traffic to production environment and 30% of traffic to the new version .


3. **Geo-location Routing policies :**
+ It routes traffic based on the geographical location of the user making the request .
+ You can specify a continent or a specific country 
+ You can associate health checks with geo-location routing policies

USE CASE : This is good when companies want to rollout new versions of their application to specific group of people for test first . 
+ This also helps in serving content based on location for better perfomance 


4. **Latency Routing Policy** 
what is latency? : It refers to the time it takes for data to move or travel from its source (location) to (another) its destination and back .
+ When data moves faster from source to destination and back its considered low latency or on the other hand (good )
+ when data moves or travel slowly from source to destination and back its considered high latency . (bad)
+ When creating Latency routing policies  you need to specify where the resource is located or have been created within your aws account 

+ Latency routing policy routes traffic to resources that provides the lowest latency based on the geographical location of the user.
+ Route 53 measures latency between its DNS servers and your resources 
+ latency routing policies supports health checks 

5. **Failover routing Policy :** 
+ This routing policy routes traffic to a specific resource under normal conditions . That is as long as the resource is healthy and available its going to route the traffic to it but if the resource becomes unavailable it will (failover)to the secondary resource you have configured .
+ Route 53 continously monitors health of the primary resource using health checks .

USE CASE : This is good for high availability because of a backup resource in case something happens to the primary. 

6. **Geo-proximity routing Policy**  : 
This routes traffic based on the geographical location of the users and resources and it allows you to adjust traffic distribution based on a bias 
+ You can change the size of the coverage using Bias and a positive bias value that is anything 1+ can increase the size of coverage and anything -0 will shrink the size coverage 

7. **Ip-based Routing Policy** : 
It routes traffic based on the ip address of the user making the request . You can create rules that specify different resources based on the user ip addresses .
+ It supports heath checks 


8. **Multi-value Answer Routing Policy** :
It routes traffic to multiple resources and returns multiple values ( such as ip addresses ) for DNS query.The DNS resolver will return selected value of its choice . 

Demo: Multi-Value
      ip based 
      failover routing 




DNS Mamangement -----> Create sub domains
Traffic Management ---> Routing policies , 
Domain Registration ---> register a domain , you import an existing domain , create hosted zones 
Health Checks -->
DNS security -->

**HEALTH CHECKS IN ROUTE 53** 


Route 53 provides health checks to monitor the health and performance of your resources .Health checks helps route 53 
to determined if a resource is available and perfoming normally , allowing users to use this information to setup failovers and routing polices based on the information. 

+ With route 53 health checks are Pulicly accessible, meaning they can monitor all types of Public Resources, but route 53 health checkers are outside the VPC  which makes it difficult for them to access private hosted zones but to go around this  you can create a cloud watch metrics associate a cloudwatch alarm , then creates a health check that checks the alarm itself . 




**TYPES OF ENDPOINT HEALTH CHECKS WE CAN CREATE** 

1. + **Endpoint Health Check ;*

 In route 53 endpoint health check verifies the availability and responsiveness of your resources by sending a request to the specified endpoint .These checks can be HTTP , HTTPS , or TCP -based and it ensures your resources are operational and serving traffic correctly . 


2. + **Calculated Health Checks (Status of other health checks)* : 

+ A calculated Health check, checks other health checks . It evaluates the health of a resources based on the results of other health checks or multiple health checks . 
+ You have the option to set when the parent health check can be deemed healthy . e.g if one out of the 3 or 4 health checks is healthy then the parent health is healthy and vice versa
+ You can use OR , AND , NOT conditions when setting this up 
+ You can monitor upto 256 Child Health Checks 
+ You can specify how many of the child health checks needs to pass before the parent pass 


3. + **State of CloudWatch alarm Health Checks*: 
This uses a cloudwatch alarm (Cloudwatch Is a monitoring tool in aws that monitors your resources within the aws ecosystem ) to determined the health of your resources . 




9: 27 pm est 



