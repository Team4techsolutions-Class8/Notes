Instructors name :  Calson 
Professional : DevOps Engineer specialist 

Partner name is ;  Ali Mohammed (Python)

TOOLS 

Communication 

+ Slack (Team communication) + Notifications (CI/CD)
+ Whatsapp (Announcement)
+ Telegram (All class recorded videos)
+ Draw.io 
+ Confluence ( Documentations )
+ Jira ( Ticketing)
+ Github (all our running notes)
+ Personal laptop or computer (highly recommended External Monitor )

-----------------------------------------------------
schedule 


Monday : 7pm EST - 10 pm EST (10 Mins break)
Tuesday: 7pm EST - 10 pm EST (10 Mins break) 
Thursday : 7pm EST - 10 pm EST (10 Mins break)

cloud : 4-5 months 
devops : 5-7 months including 1 month 


AWS / Linux , Bash , Terraform , ANSIBLE 
AZURE 



make up classes 

30 mins + Ask Questions from previous 


AFTER THE COURSE WHAT HAPPENS 

+ Linkedin profiling 
+ Resume profiling 
+ Show to apply for jobs ( AI)
+ Bootcamp session (1 month) - Techniques ) recorded videos 
+ Interview support  

---------------------------------
What type of OS you need for the course 

+ 13 inch 
+ Windows 
+ MacOs 

do not use a chromebook 


classes 

Monday : 






ROAD MAP FOR OUR DEVOPS PROGRAM 

+ Linux (Operating System)
+ Bash 
+ Git & Github 
+ AWS (Amazon Web Services) 
+ Terraform (IAC)
+ Ansible 
+ Jenkins 
+ Maven 
+ Docker 
+ Sonarqube 
+ Nexus
+ Kubernetes 
+ Helm Chats 
+ Prometheus and Grafana 
+ Nginx 
------------------------------

cloud engineers 
------------------

+ Linux (Operating System)
+ Bash 
+ Git & Github 
+ AWS (Amazon Web Services) 
+ Terraform (IAC)
+ Ansible 


9:17 pm est 

What is DevOps 
What is Cloud computing 



What is a software ?: Collection of programs,data , instructions that helps a computer or device to perform specific task 

1. System software e.g Windows , MacOs , Linux 
2. Application software (Mobile application and web application )

Business of Software development ----> 

Development Team ( Cross functional Team)



What is SDLC (Software Development Life Cycle)?

+ Plan (All development team)
+ Analysis 
+ Design ( UI/UX designers)
+ Implemenation ( Developers , DevOps , Cloud engineer )
+ Testing ( Q.A , DevOps )
+ Deploy (users can now use the software)
+ Maintenance 



Waterfall Methodology : ( Delivery delays ,Adaptability is not possible , customer is not engaged early in the development ,  )

100 m


Agile Methodology | scrum framework 

+ Sprint Planning 
+ Daily standups 
+ Sprint Retrospect 
+ Demo Session 


Agile Methodolgy ---> 

20 requirements : features 

sprint planning  ----> front end , sign up , send money via email 
2 weeks 
cloud engineers :  build infrastructure 




Schedule 

Monday : 7pm -10 pm est 
Tuesday: 7pm -10 pm est 
Thursday : 7pm -10 pm est 



Wednessday : 7Pm - 10 pm est 
Friday : 7pm - 10 pm est 
saturday : 10 am - 1 pm est 


iterative approach (scrum framework----> daily standup , Sprint planning , Sprint Retrospect  , Kanban )


Sprint Planning : 
Daily Standup : 


**Cloud Computing**

**Types of Computers** 


## Personal Computer (PC) : 
General Purpose e.g desktop ,Laptops, tablet 

## works stations : 
Work stations are high perfomance computers specialized task , graphic designing etc

## servers : 

 Servers are computers designed to provide services or resources to other computers over a network . eg web hosting , file storage 

## Mainframes :
   Large and power machines they are capable of handling large amount of data (information) and it can support many users at the same time .

## Super Computers : 
  These are fast and power computers 



**Computers can not function without an Operating System ( Computer Software)**

## Operating Systems

Windows Operating system   :  owned by Microsoft 
MacOs Operating System     :  Apple 
ChromeOS                   : Google
Linux Operating System     : Open source ( Technology ) - Linus Torvalds  ( Unix-like )



## Phones  operating system (software)

Android       : 
IOS    :
Chrome IOS 

## Open Source : 
You do not pay to use it , Source code is available for use or remodification 



# Vendors : they create and sell computers 

Toshiba :   Windows        
Lenovo :  windows 
Apple : MacOs
Dell : windows
Asus ; windows
Acer: windows
Samsung: windows
Panasonic : windows 
IBM : windows

`Third party Vendor` 

Walmart 
Best Buy 
The Source 



# Parts of a Computer 

## CPU : 
Central Processing Unit ; This is like the brain of your computer .Its performs instructions and calculations , Process information (data) and control operations of other things in the computers .

## Memory (RAM) : Random Access Memory :

A ram temporary stores information (data) , and information the cpu needs to quickly access 

## Storage Devices :
They permanently store information (data)
- HDD ( Hard Disk Drive) : 
- SDD ( Solid State Drive) : It uses flash memory to store information it offers faster read and write 
- Optical Drives : 

## Graphical Processing Unit 
This is responsible for graphic display on your computer 

## MotherBoad 

## Input Devices : It allows users to interact with the computer input data 
- Keyboard 
- Mouse 
- Touchpads 
- Touch screens 

## Ouput Devices : 
This displays data or information processed by the computer 

- Monitor 
- Printers 
- Speakers 

## Networking Devices : Modems , routers , network interfaces .





# CLOUD COMPUTING



`Traditional I.T infrastructure` : All I.T equipments like compute , networking , storages devices etc are stored in a physical location and managed by the I.T team 


`Cloud Computing` :  Cloud computing is an on demand delivery of I.T resources / services over the internet with a pay as you go pricing model instead 
of buying , owning and maintaining physical data centers and servers .


**Cloud Providers** 
These are the companies or orgnization that provides us the services to be able provision I.T infrastructure over the internet 

**Different Cloud Providers**

+ AWS ( Amazon Web services) 
+ Microsoft Azure 
+ GCP ( Google Cloud Platform)
+ IBM Cloud
+ Oracle Cloud 
+ Alibaba Cloud 
+ Digital Ocean 
+ Salesforce 



## Benefits of Cloud Computing 

+ **Cost Efficiency** : pay as you go model. you only pay for resources or services you use .This reduces upfront capital expenditure . (CAPEX )0
+ **Cloud is Scalable (out / in) and Flexible** : Cloud services provides on demand resource allocation , allowing businesses to easily scale up that is increase their resources , or you can decrease your resource at anytime .This beneficial to companies with fluctuating workload  
+ **Disaster Recovery and Business Continuity** : Cloud providers have robust / strong / resilient disaster recovery solutions .
+ **Security** : It has robust security services monitoring , WAF(web application firewall) , Networking 
+ **Maintenance and Upgrades are easy** : Most of the time the service provider is responsible for maintaining , managing and updating the infrastructure . This reliefs businesses from system maintenance , allows them to focus on the main activities . 
+ **Accessibility** : Cloud services can be accessed from anywhere with an internet connection, faciliating remote work and collaboration. 




**What is On premise ?**

This refers to the traditional model where companies or organisations manage their I.T infrastructure  e.g storages , networking hardwares , softwares etc in a physical location or onsite .The companies or organisations are responsible for maintaining the hardware, software , security and other aspects of the I.T environment .

**Benefits or advantages of having your I.T infrastructure on premise**

+ **Control and Customization on premise is good** . Organisation can customize and control their resources to meet different requirments 
+ **Security and compliance** : The cloud environment is secured but there are some strict complaince or regulations hinders companies to move to the cloud due to data concerns so having their data on premise provides more security to them.
+ **Performance** : On premise solutions can offer better perfomance to certain applications , as the data do not need to travel over the internet.This is good for applications which are sensitive to latency .
+ **Offline Accessibility** : On-premise solutions might not require internet connections for access , making them suitable for environments with unsuitable internet services .





 ## Deployment Models in Cloud Compting 

 This are specific environments where your services in the cloud are going to be hosted . 

1. **Public Cloud**

 Public cloud services are provided over the Internet and offered by third-party providers. These services are available to anyone who wants to use or purchase them. The provider owns, manages, and assumes all responsibility for the data centers and infrastructure. Examples include Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP).

----> Apartment Building  (Public Cloud) ------> tenants  (user) ------> rent apartment (cloud services) , gym , swim pool , massage parlour , (storage , servers, networking ), you dont own build  (infrastrure) you pay only when you use it . 


2. **Private Cloud** 

A private cloud is dedicated to a single organization and can be hosted internally or by a third-party provider. The infrastructure and services are maintained on a private network, and the hardware and software are dedicated solely to your organization. This model offers more control and security over your resources.

Single family home , everything within the property is yours ,control over modifications 


3. **Hybrid Cloud** 

Hybrid cloud combines public and private cloud elements, allowing data and applications to be shared between them. This model provides businesses with greater flexibility and more deployment options by allowing workloads to move between private and public clouds as computing needs and costs change.




**Types of Services offered by Cloud Providers ( aws , azure ,google cloud )**

1. `Infrastructure as a Service (IaaS)` :  These providers offer basic compute, storage, and networking resources on-demand, from the cloud, with scalable and elastic capabilities. Users have control over operating systems, storage, and deployed applications but do not manage the underlying cloud infrastructure. Examples of IaaS include physical and virtual servers, storage, and networking features.

2. `Platform as a service (PaaS)`: PaaS providers offer a cloud-based platform that allows developers to build, deploy, and manage applications without dealing with the underlying infrastructure. This service is geared towards software development, providing tools to support the complete web application lifecycle: building, testing, deploying, managing, and updating. PaaS includes services like web servers, development tools, and database management systems.

3. `Software as a service (Saas)`: Software as a Service (SaaS): SaaS providers offer software applications over the Internet, typically on a subscription basis. With SaaS, users can access and use software applications hosted on cloud infrastructure via web browsers without worrying about installation, maintenance, or infrastructure. This model includes applications like email, customer relationship management (CRM) systems, and collaboration tools.

4. `Infrastructure as code`(IAC) : This is a service where you can deploy your infrastructure using code . 



# AWS  (Amazon Web Services )

Amazon Web Services (AWS) is a comprehensive and widely adopted cloud platform offered by Amazon. Launched in 2006, AWS provides a variety of scalable and flexible cloud computing solutions to individuals, companies, and governments. Its extensive offering includes over 200 services available from data centers globally. These services are designed to help businesses scale and grow by providing powerful computing power, database storage, content delivery, and other functionality.

some services 

+ compute 
+ containers
+ database 
+ Networking 
+ Developer tools 
+ Machine Learning 



**What is a Data Center** : 

A data center is a centralized facility or a Physical location used by organizations or company to house/keep and manage their IT infrastructure, including servers, storage systems, networking equipment, and other critical components for storing, processing, and disseminating data and applications


AWS has its own data centers and they offer the cloud services over the internet from this phyiscal locations . customers , companies ,organisations store their data in this physical locations and they refer to their data centers as `AVAILABILITY ZONES` 



**What is a region** : 

In Amazon Web Services (AWS), a region is a geographical area where AWS has multiple data centers. Each region is designed to be completely isolated from other regions to ensure fault tolerance and stability. AWS operates multiple regions globally, and each region consists of multiple availability zones.

+ Each region is identified by a unique name (e.g., `virginia (us-east-1a , us-east-1b ,us-east-1c) , eu-west-1).canada is called ca-central-1` 
+ Regions are composed of multiple availability zones.
+ AWS services are typically deployed globally across multiple regions to provide redundancy and improve performance for users located in different geographic locations.





**WHAT IS AN AVAILABILITY ZONE (AZ)**

An availability zone (AZ) is one or more discrete data centers within a region. These availability zones are interconnected through low-latency links, but they are physically separated from each other. This physical separation is important for providing redundancy and resilience. If one availability zone within a region experiences a failure, the other availability zones in the same region can continue to operate independently, ensuring `high availability` and `fault tolerance` for applications and services hosted on AWS.

+ Availability zones are isolated from each other to minimize the impact of failures and improve `fault tolerance`.
+ They are connected through high-speed, low-latency links to enable synchronous replication and data transfer between availability zones.
+ Deploying applications across multiple availability zones within a region enhances fault tolerance and high availability by ensuring that failures in one availability zone do not impact the availability of applications hosted in other availability zones.

Region: us-east-1
availability zones : unique names of the data centers us-east-1a , us-east-1b , us-east-1c , us-east-1d





## Why did AWS introduce availability Zones?


+ **Fault Tolerance** : By distributing resources across multiple availability zones within the same region, applications can withstand failures in one availability zone without impacting availability. This redundancy ensures that if one availability zone experiences an outage, services can continue to operate from other availability zones.

+ **High Availability**: Applications deployed across multiple availability zones can achieve higher levels of availability because they are not dependent on a single data center or infrastructure. Even if one availability zone becomes unavailable due to maintenance, hardware failures, or other issues, the application can remain accessible from other availability zones.

+ **Improved Performance**: Availability zones are interconnected through low-latency links within the same region. This allows for synchronous data replication and enables applications to achieve low-latency communication between components deployed across different availability zones, enhancing overall performance.

+ **Scalability**: Availability zones provide scalability by allowing organizations to distribute workloads across multiple zones within a region. As demand for resources grows, organizations can easily scale their applications horizontally by adding more instances or services across different availability zones

+ **Disaster Recovery**: Availability zones facilitate disaster recovery by providing geographically separated infrastructure within the same region. Organizations can use availability zones to replicate data and resources across different zones, enabling them to quickly recover from disasters or regional outages without data loss or downtime.

+ **Compliance and Data Residency**: Some regulatory requirements mandate data residency within specific geographic regions. Availability zones allow organizations to ensure compliance with these requirements by deploying resources in designated regions while still benefiting from the redundancy and fault tolerance provided by multiple availability zones.



**Virtualization** 



**What is Virtualization** : Cloud providers like aws , azure , gcp are using the technology called Virtulization to offer I.T services over the internet. Virtualization in AWS (Amazon Web Services) or cloud computing refers to the practice of creating virtual instances of computing resources such as servers, storage, networks, and applications, which are then delivered over the internet. This allows users to access and utilize these resources on-demand without the need for physical hardware infrastructure.

In AWS, virtualization is achieved through technologies like Amazon Elastic Compute Cloud (EC2) (computers or servers you can create on aws) for virtual servers, Amazon Simple Storage Service (S3) for virtual storage, and Amazon Virtual Private Cloud (VPC) for virtual networks. These services abstract the underlying hardware and provide users with scalable and flexible computing resources that can be easily provisioned and managed via a web interface or APIs.


**Virtual Machines (VMs)**: A VM is a software-based emulation or imitation of a physical computer that runs its own operating system and applications. Virtual Machines are created and managed by the hypervisor and can be provisioned, cloned, migrated, and scaled as needed.





## Virtualization Types:**

### Hardware Virtualization ; 

Hardware virtualization involves creating a virtual machine that acts like a real computer with an operating system. This type is often used to consolidate multiple physical machines into fewer hardware servers, saving space and resources.


**Full Virtualization** : In full virtualization, the guest operating system (This is the computer you will create from the main computer (host computer)  runs unmodified on the virtual machine, while the hypervisor handles hardware emulation and resource management.

**Para-virtualization**: In para-virtualization, the guest operating system is modified to be aware of the virtualization layer, which can improve performance and efficiency.

**Hardware-assisted Virtualization**: Modern processors include hardware features that accelerate virtualization, such as Intel VT-x and AMD-V, which enhance the performance and security of virtualized environments.


**Types of Hypervisors** 

Bare-Metal Hypervisors: This are physical hardware 
+ VMware vSphere/ESXi, Microsoft Hyper-V, and Citrix XenServer.

Hypervisors (Hosted Hypervisors)  : Oracle VirtualBox, VMware Workstation, and Parallels Desktop.




+ Citrix Xen (Xenserver) : An Opensource Hypervisor ( software)
+ Vmware ESXI 
+ Microsoft Hyper-V  : windows 
+ KVM ( Kernel based virtual machine)




## Virtualization Benefits:

1.  **Cost Efficiency** : 

+ Virtualization allows multiple virtual systems to run on a single physical system, reducing the need for multiple hardware units. This consolidation significantly reduces hardware costs.

+ It serves energy because eliminates physical servers that could have been consuming electricity or energy . It lowers operating costs but also helps in reducing the carbon footprint of data centers.

2.  **It reduces resource Utilization**: 

+ Efficient Use of Hardware: Virtualization allows multiple virtual machines (VMs) to run on a single physical machine, maximizing hardware utilization.
+ Reduced Hardware Costs: By consolidating servers, organizations can reduce the need for physical hardware, leading to cost savings.

3. **Scalability and Flexibility** : Virtual environments make it easy to scale up or down based on demand . that is increase resources for example computer or servers based on demand and easy provision (easy to create)

4. **Isolation and Security** 

+ Enhanced Isolation: Each VM(virtual machine ) operates independently, providing isolation between applications and services, which enhances security.
+ Controlled Environment: Virtualization allows for controlled environments for testing and development without impacting the production environment.

5. **Disaster Recovery and High Availability**

+ Simplified Backup and Recovery: Virtualization simplifies the backup process by enabling snapshots of VMs, making disaster recovery more efficient.
+ High Availability: VMs can be moved between physical servers without downtime, ensuring continuous availability.



**Take note : AWS services | resources are categorized based on their scope and reach within the aws Infrastructure or ecosystem** 

1. **Regional Services* : 
These are services that operates within a specific aws region .

+ They are confied to a specific geographical area of a single AWS region which may contain Multiple availability zones 
exmples of regional services can be `VPC, Amazon EC2 , Amazon RDS , AWS lambda function , SNS etc 
**Usage*: These services are used when data residency and latency requirements are tied to a specific geographic location.


2. **Global Services*: 
Services that operate across multiple AWS regions simultaneously.

+ They are designed to provide a unified view and functionality regardless of the geographic region.
+ Examples of these services in aws IAM (Identity and access management),Route53 , Aws cloudfront , S3 buckets 

3. **Zonal Services*

+ Services that operate within a specific availability zone within an AWS region.
+  They are limited to a single availability zone, providing high availability and fault isolation.
+ Examples of these services EBS Volume (Elastic Block Store), EC2 Instances .
+ These services are used for applications that require low latency within a single availability zone or need fault isolation within a region.

| Service Type    | Scope                          | Examples                                                      |
|-----------------|-------------------------------|---------------------------------------------------------------|
| **Regional**    | Operate within a single region | Amazon VPC, Amazon EC2, Amazon RDS, AWS Lambda                |
| **Global**      | Operate across multiple regions| Amazon Route 53, AWS CloudFront, AWS IAM, Amazon S3 (global)  |
| **Zonal**       | Operate within a single AZ     | Amazon EC2 instances, Amazon EBS volumes                      |



FIRST AWS SERVICE 

# IAM : Identity and Access Management 

+ This is a Global Service in AWS 
+ This is one of the core services provided by AWS 
+ We can classify it under security 

**Definition** : IAM stands for Identity and Access Management, and it's a core service provided by Amazon Web Services (AWS) that enables you to manage access to AWS resources securely. IAM allows you to control who can access your AWS resources (authentication) and what actions they can perform on those resources (authorization).



**Key Features of IAM**  

+ User Management : You can create and manage users and groups , assign permissions to allow access to resources using IAM . 

**User** : Represents a person or service that interacts with AWS . Users have permanent credentials and are assigned permissions through policies.

User can have access or use aws using two methods .

+ AWS management console (Graphical User Interface)
+ Terminal (CLI : Command line interface)


To setup commandline interface as a user using the commandline 

Step 1 :

Install the aws cli on your computer . If you have a MacOs , on the launch pad open and search terminal , for windows on the start open and search `command prompt` or `powershell` 

step2 

Install aws cli 
follow this documentation to install base on your Operating system : https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html 

Step 3 

The user you created click on the user and under the user click on security credentials.
Look for Access Keys and click on create access keys 
after creating the access keys configure the keys by using the following commands on the CLI 

step3 

type : `aws configure` 

and follow prompt and fill the following information 
that is the access and secret key including your default region name and output format which will always json 

AWS Access Key ID []: AKIA47CR2IN2VU5MUERN
AWS Secret Access Key []: yn/ETZE9J8QVNlOdijXPBc7KrtidyxarGL7SdW7V
Default region name []: ca-central-1
Default output format []: json


When you create user on aws we refer them as `IAM user` 
Two ways you can login on your aws account using the console :  `Root user`(this is user who created the aws acct) or Iam user 
when you create yOur aws account it comes with the root user by default 
As security best practise it is highly recommended that you should never login with your root user instead use IAM users

**What are security best practises** 

+ best practise aws recommends that all access keys be rotated every 90 days 
+ As security best practise do not use the root account instead create an IAM user and login with the user . 




## WHY SHOULD YOU USE `IAM USER` INSTEAD OF `ROOT USER` IN AWS ? 

+ **Principle of Least Previllage** : Root users have a lot of unrestricted access to all resources on your aws account which escalated permission if there was security breach it will impact your account and resources . 

USE IAM USER where you can grant them permission to carry out specific task , if this user was compromised there will not be too much impact in your account and resources . 

+ **Reduces security Risk** : 
+ Root account is high risk , if the root user is compromised by an attacker the will have access to all resources 
+ Iam users have lower risk : If this user was compromised the will be damaged only on the resources the user had access to 

+ **Accountability & Auditing** :  

+ Using the rootuser can lead to Lack of accountability : Actions performed by the root user are harder to attribute to a specific individual, making it difficult to track changes and identify issues.
+ On the other hand when you use an IAM user you can monitor individual actions , logings and you audit all these through a service in aws called Cloudtrail . 

+ **Operational Safety** : 

+ using the root user can lead to accidental misused whereby you can unintentionally delete critical resources .Using it everyday leads to risk 
+ whereas using iam users have controlled access and permissions reducing the risk of accidental delete of resources .



**Group** :
`What is a group` ? : A group is a collection of users, you can use groups to assign permissions to `multiple users simultaneously` 

+ in a company you can have different groups within the development team e.g ( DevOps , Cloud Engineers , Quality Assurance engineers , UI/UX etc)
+ its a good practise to assigned permission to users in groups as much as possible . 
+ When you give a permission to a group , all users under that group automatically inherit the permissions . 
+ When creating a group you can assign existing users to the group and permissions & you can also create an empty group 
+ a user can be in multiple groups at the same time 




## Benefits of Managing Permissions Using Groups in `IAM `

1.  **It simplifies permission management** : 
- you have a centralized control of permissions this is possible because by attaching a policy to the group it automatically applies to all the users in the group . 
- Assigning users to groups and attaching a permission to the group makes sure there is consistency across all users.All users wil have the same permissions unlike if i wanted to manually assign permissions to users you could make a mistake . 

2. **Managing users using Groups makes it Scalable**
- Its easy to Onboard ,When a new user is added to the company adding them to the group speeds up their onboarding process
- easy to offboarding (temporary when a user is on vacation or permanently if the user is no longer employed) when someone is leaving removing them from the group revokes their permission 

3. **Enhanced security** : 
- Using groups will lead to enhancing security by following the principle of least previllage accessed
_ It makes it easy for auditing . Auding group is easier than auting individual users 

6. Improved Efficiency ( time saving )







# Policies : 

Its a Json Document that defines permissions.This document specifies which actions are allowed or denied on which resource or resources and under what conditions a user can manage them . 
+ This is you authorizing a group , user or service to manage your resource or resources in aws . 


Polices are simply permissions , but this permissions are being granted or denied using what we call a json document .In this document we defined if the permission should allow a user or group of users to carryout a spefic action on a resource or resources 



**Different Types of Policies**

# MANAGED POLICIES

1. **AWS managed Policies** : 

+ These are policies that comes with your aws account by default 
+ AWS manages these policies that is why its called AWS managed 
+ With aws managed policies you can not delete the policy and you also can not modify the policy
+  You can attached the policy to multiple users and groups or role 

2. **Customer Managed Policies**: 

+ This is a policy that is created by the user and the user manages the policy 
+ You can attached the policy to multiple users and groups or role 
+ You can reuse the policy and you can delete or modify the policy 


###############################

**Inline Policy**: 

+ This is a policy created by the user and managed by the user 
+ You can only directly attached it to a single user , group or role 
+ They provide specific permissions to a particular entity ( user , group , service) which makes it not resuable ) 
+ Use inline policies only if you think no other user will need that permission in the future because you cant reuse the policy 




DIFFERENCE BETWEEN MANAGED POLICES AND INLINE POLICIES

| Feature                   | AWS Managed Policies                    | Customer Managed Policies               | Inline Policies                          |
|---------------------------|------------------------------------------|-----------------------------------------|------------------------------------------|
| **Definition**            | Predefined policies created and managed by AWS. | Policies created and managed by the customer. | Policies embedded directly into a user, group, or role. |
| **Management**            | Managed by AWS; automatically updated by AWS to include new services and actions. | Managed by the customer; allows for customization to meet specific needs. | Managed directly within the user, group, or role it is attached to. |
| **Reusability**           | Can be attached to multiple users, groups, or roles across the account. | Can be reused across multiple users, groups, or roles within the account. | Specific to a single user, group, or role and cannot be reused. |
| **Modification**          | Cannot be modified by customers.         | Can be created, updated, and deleted by customers. | Can be edited directly within the user, group, or role it is attached to. |
| **Examples**              | `AdministratorAccess`, `AmazonS3ReadOnlyAccess` | Custom policies created by the customer. | Inline policies specific to a particular entity. |
| **Use Case**              | Common permissions needed across many AWS accounts. | Specific needs not covered by AWS managed policies. | Specific, non-reusable permissions tailored to individual users, groups, or roles. |
| **Complexity**            | Simple to use; AWS handles updates and maintenance. | Requires customer to manage and update as needed. | Simple to assign but can become complex to manage at scale. |
| **Policy Scope**          | Broad and generic to cover common use cases. | Can be tailored to precise needs and security requirements. | Very specific to the needs of a single entity. |
| **Best Practices**        | Use for general permissions where AWS managed policies suffice. | Use for more granular control where specific permissions are needed. | Use sparingly for specific cases where managed policies are not suitable. |

recommendation : If you using permissionsets to grant access use more inline policies 
if using IAM : use more of customer managed policies 


# Policies : 

Its a Json Document that defines permissions.This document specify which actions are allowed or denied on which resource or resources and under what conditions a user can manage them . 
+ This is used to authorize groups , user or service to manage your resource or resources within your aws account. 


**Example of a Json Policy**  the full meaning of json is -->  Javascript object notation   

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:GetObject",
                "s3:ListBucket",
                "s3:DeleteObject"
            ],
            "Resource": [

            ]
        }
    ]
}

--------


wildcard 


{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "s3:*",
            "Resource": [
                "arn:aws:s3:::your-bucket-name",
                "arn:aws:s3:::your-bucket-name/*"
            ]
        },
        {
            "sid" :denys3actions
            "Effect": "Deny",
            "Action": "s3:*",
            "Resource": [
                "arn:aws:s3:::your-bucket-name",
                "arn:aws:s3:::your-bucket-name/*"
            ],
            "Condition": {
                "NotIpAddress": {
                    "aws:SourceIp": "192.0.2.0/24"
                }
            }
        }
    ]
}





**Version**: This is the policy language version you are using 
**Statement** ; A policy can have one or more statements and each statement describe a specific permission 
  + **Effect** : The effect specifies if the policy is allowing or denying access ( Allow or Deny) to access a service or resource
  + **Action** This are actual actions/permissions/ task a user can carryout on the resource they have been allowed to have access to 
**resource** : This is a service or object or resource to which the actions are applied to
**Sid** : This is a description used to distinguish different statement policies
**condition** : This defines when a policy is in effect. These are optional 



HANDS ON 
Creating an IAM policy from scratch 

There are two policy editors we can use to create IAM policies

1. Visual Policy Editor
2. Json Policy Editor 



**RECAP : QUESTIONS AND ANSWERS** 

policies are permissions that can be assigned to users or groups 
gives access to someone to do a task in aws 
its json that accepts or deny users to do some specific task 


+ Inline Policies 
+ Managed Policies (AWS managed policies and Customer managed policies)


Best Practise 

+ Follow Principle of Least Previllage 
+ Avoid using the root user and instead user IAM USERS 
+ Access keys should be rotated maximum 90 days 
+ Temporary Suspend unused keys especially if an employee is on vacation for 2 + weeks 
+ Used groups as much as possible when managing identity and access management 
+ Avoid using wild card 
+ Set up strong password policies for your iam users this prevents poor passwords which could lead to compromise 
+ Set up IAM user password rotation 90days preventing IAM users from reusing the same password 



**IAM ROLES**

**IAM ROLES** : Its a set of permissions an AWS Services ,User , Groups assume temporary which allows or deny which actions they can perform on a resource or resources. 
Roles are good for resource to resource authorization. This means if you want two resource to share data for authorization its highly recommended to use IAM ROLES instead of IAM POLICIES.IAM roles unlike an IAM USER that has access keys , password attached to them IAM roles do not have any long term credentials attached to them

Iam policies are longlived method of authorizing someone in ur aws account 
iam roles on the other are short live permissions to authorized a user ,groups in ur account 

IAM ROLE can be associated to a user , resource or used for cross accoubnt permissions 


+ Temporary Credentials 
+ Flexible 
+ You can delegate roles to users , service etc without long term credentials


`Trusted Entity` : A trusted entity is who is going to assume or use the role. 

entities

+ AWS service (ec2 , s3 , cloudfront ,cloudwatch)
+ AWS Account (891377304437 )
+ Web identity 
+ SAML 2.0 Federation 
+ Custom Trust Policy 



**Trust Policy** : This policy defines who can assume / use the role .
In the trust policy the service that assumes the role is what we call the `principal`  

example of a trust policy 

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "ec2.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}


**Understanding the Trust Policy for IAM ROLES** 

`version` : policy version 
`statement`: This is the permissions define in the trust policy  
`Effect` : This grants permission to allow or deny a user or service to assume or use the role 
`Principal` : This is simply the service ,user , account who will assume or use the role 
`action` : This defines the action the trust policy wants the entity to do 


** DIFFERENCE BETWEEN IAM ROLE AND IAM POLICY **    


| Feature               | IAM Role                                                                 | IAM Policy                                                                 |
|-----------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------------|
| **Identity**          | An IAM role is an AWS identity with specific permissions.                | An IAM policy is a JSON document that defines permissions.                |
| **Usage**             | Intended for use by applications, AWS services, or users needing temporary access. | Policies are attached to IAM identities (users, groups, roles) to grant permissions. |
| **Trust Policy**      | Has a trust policy that defines who can assume /use the role.                 | N/A                                                                       |
| **Credentials**       | Provides temporary security credentials when assumed.                   | N/A                                                                       |
| **Attachment**        | Roles are not attached to users but assumed by entities needing the role. | Policies can be attached to users, groups, or roles.                      |
| **Types**             | N/A                                                                      | Managed Policies and Inline Policies                                      |
| **Permissions**       | Permissions are defined by the role's attached policies.                 | Defines what actions are allowed or denied on specified resources.        |



`Identity Provider`

Identity Providers (IdPs) in IAM refer to external systems that allow users to authenticate to AWS. AWS Identity and Access Management (IAM) supports several ways to establish trust with identity providers for authentication, including using web identity federation and Active Directory. This enables users to log in to AWS using credentials from an external provider, such as Google, Facebook, or corporate directories.

we have SAML 
OPENID CONNECT 


**Account Settings**
This will help you to Configure the password requirements for the IAM users
Password policy : 



# ACCESS ANALYZER  : 

This is a feature in IAM that helps you to identify resources in your aws accounts and organizations such as Amazon S3 buckets or IAM roles, that are shared with an external entity. It uses logic-based reasoning to analyze resource-based policies, IAM policies, and other access control mechanisms to provide a detailed report on resources that are accessible from outside your AWS account.

+ `Resource Analysis`: It identify the resources shared with external entities 
+ `Policy validation`: It checks your policies to ensure you are not granting unintended access & it provides recommendations to enhanced your security posture in your aws account  
+ `Continous Monitoring` : Monitors changes in your resource policies & updates the findings  
+ `Displays detailed findings` : it displays information about the external entity , the resource which helps you to understand the level of access the external entity has 


`external access`: Analyzing external entities(services) having access to your aws resources 
`unused access` : This are access that have not been active for a number of days . 


## Benefits of Using Access Analyzer

+ It Improves security by helping you to identify any access with external entities and helps you to mitigate or prevent any unintended access to your aws resources 
+ It helps you to identify any unintended resource sharing 
+ It ensures that your resource policies are correctly configured and do not expose your resources by giving you recommendations 
+ It helps in compliance , it provides information or insights into access configurations (SOC2)
+ It generates IAM policies based on Access activity 


## Credential Report 

Credential Report gives you an overview of your iam users and their settings , activities , access levels etc . some of the things you will see in the report
+ User information 
+ Password information 
+ Access Key information 
+ MFA (Multi Factor Authentication) information 
+ Security access information 

**WHY IS IT IMPORTANT TO USE CRENDENTIAL REPORT**

+ It helps you in auditing your iam users this helps to understand user credentials and usage 
+ It helps in securitty monitoring it gives you an overview of your iam users security credentials statuses allowing you to proactive protect your aws account . e.g you can see if iam users have not set up MFA (Multi factor authentication). 
+ It helps you with user management . it helps you to identify any security risk especially when there is outdated credentials or inactive users you can simply see and remove them . 


**MFA**

MFA (Multi Factor Authentication): In IAM MFA'S adds an extra layer of security for users who have access to your aws account . 

+ Increased security because a user after putting in their username and password will still need to confirm the MFA code before it allows them to login . Which means if their password was compromised the attacker will need to have access to their device e.g cell phone before they can login into the account 

+Some Supported Devices you can used for MFA are ( Virtual MFA device e.g google authenticator, Microsoft Authenticator, Duo Mobile, or Authy app  )


summary of iam 


So far we have been learning on how to manage and setup iam users within our aws console 
but we can also create iam policies ,users , user groups through the commandline 



all aws commands starts with `aws` this helps to send api calls to your aws account . 

aws = command to interact with aws api 
resource = s3 ,ec2 , iam ,cloudformation 
action = what you need e.g ls means list 


**AWS IAM User Management Commands**:

Create User
`aws iam create-user --user-name MyUser`
Use Case: Create a new IAM user named MyUser.

Delete User

`aws iam delete-user --user-name MyUser`
Use Case: Delete the IAM user named MyUser.

List users 
`aws iam list-users`
Use Case: List all IAM users in the account.


Update User
`aws iam update-user --user-name MyUser --new-user-name NewUserName`
Use Case: Rename an existing user from MyUser to NewUserName.
Get User


`aws iam get-user --user-name MyUser`
Use Case: Retrieve details for a specific user named MyUser.
Create Login Profile


`aws iam create-login-profile --user-name MyUser --password MyPassword`
Use Case: Create a password for the IAM user MyUser to allow console access.


**Group Management Commands:**

Create Group
`aws iam create-group --group-name MyGroup`
Use Case: Create a new IAM group named MyGroup.


Delete Group
`aws iam delete-group --group-name MyGroup`
Use Case: Delete the IAM group named MyGroup.


List Groups
`aws iam list-groups`
Use Case: List all IAM groups in the account.
Add User to Group

see more command below on the table 

### User Management Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam create-user --user-name MyUser` | Create a new IAM user | Create a new IAM user named `MyUser`. |
| `aws iam delete-user --user-name MyUser` | Delete an IAM user | Delete the IAM user named `MyUser`. |
| `aws iam list-users` | List all IAM users | List all IAM users in the account. |
| `aws iam update-user --user-name MyUser --new-user-name NewUserName` | Update an IAM user's name | Rename an existing user from `MyUser` to `NewUserName`. |
| `aws iam get-user --user-name MyUser` | Get details for an IAM user | Retrieve details for a specific user named `MyUser`. |
| `aws iam create-login-profile --user-name MyUser --password MyPassword` | Create a login profile for an IAM user | Create a password for the IAM user `MyUser` to allow console access. |

### Group Management Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam create-group --group-name MyGroup` | Create a new IAM group | Create a new IAM group named `MyGroup`. |
| `aws iam delete-group --group-name MyGroup` | Delete an IAM group | Delete the IAM group named `MyGroup`. |
| `aws iam list-groups` | List all IAM groups | List all IAM groups in the account. |
| `aws iam add-user-to-group --user-name MyUser --group-name MyGroup` | Add a user to a group | Add the user `MyUser` to the group `MyGroup`. |
| `aws iam remove-user-from-group --user-name MyUser --group-name MyGroup` | Remove a user from a group | Remove the user `MyUser` from the group `MyGroup`. |

### Role Management Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam create-role --role-name MyRole --assume-role-policy-document file://policy.json` | Create a new IAM role | Create a new IAM role named `MyRole` with the specified trust policy. |
| `aws iam delete-role --role-name MyRole` | Delete an IAM role | Delete the IAM role named `MyRole`. |
| `aws iam list-roles` | List all IAM roles | List all IAM roles in the account. |
| `aws iam attach-role-policy --role-name MyRole --policy-arn arn:aws:iam::aws:policy/MyPolicy` | Attach a policy to a role | Attach a managed policy to the role `MyRole`. |
| `aws iam detach-role-policy --role-name MyRole --policy-arn arn:aws:iam::aws:policy/MyPolicy` | Detach a policy from a role | Detach a managed policy from the role `MyRole`. |

### Policy Management Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam create-policy --policy-name MyPolicy --policy-document file://policy.json` | Create a new IAM policy | Create a new IAM policy named `MyPolicy` with the specified policy document. |
| `aws iam delete-policy --policy-arn arn:aws:iam::account-id:policy/MyPolicy` | Delete an IAM policy | Delete the IAM policy named `MyPolicy`. |
| `aws iam list-policies` | List all IAM policies | List all IAM policies in the account. |
| `aws iam attach-user-policy --user-name MyUser --policy-arn arn:aws:iam::aws:policy/MyPolicy` | Attach a policy to a user | Attach a managed policy to the user `MyUser`. |
| `aws iam detach-user-policy --user-name MyUser --policy-arn arn:aws:iam::aws:policy/MyPolicy` | Detach a policy from a user | Detach a managed policy from the user `MyUser`. |

### MFA Management Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam create-virtual-mfa-device --virtual-mfa-device-name MyMFADevice` | Create a new virtual MFA device | Create a new virtual MFA device. |
| `aws iam enable-mfa-device --user-name MyUser --serial-number arn:aws:iam::account-id:mfa/MyMFADevice --authentication-code-1 123456 --authentication-code-2 654321` | Enable an MFA device for a user | Enable the MFA device for the user `MyUser`. |
| `aws iam deactivate-mfa-device --user-name MyUser --serial-number arn:aws:iam::account-id:mfa/MyMFADevice` | Deactivate an MFA device for a user | Deactivate the MFA device for the user `MyUser`. |
| `aws iam delete-virtual-mfa-device --serial-number arn:aws:iam::account-id:mfa/MyMFADevice` | Delete a virtual MFA device | Delete a virtual MFA device. |

### Credential Report Commands

| Command | Description | Use Case |
|---------|-------------|----------|
| `aws iam generate-credential-report` | Generate a credential report | Generate a report detailing the status of user credentials. |
| `aws iam get-credential-report --output text --query Content | base64 -d > credential_report.csv` | Retrieve the credential report | Retrieve and save the credential report in CSV format. |




**IAM BEST PRACTISES**

+ `Follow Principle of Least Previllage` :Grant only the minimum permissions necessary for a user, role, or service to perform their job.
+ `Avoid using the root user and instead user IAM USERS` :Avoid using the root user for everyday tasks. Create IAM users with only the required permissions and assign roles to the root account for administrative tasks when necessary
+ `Access keys should be rotated maximum 90 days`: Regularly rotate access keys for IAM users and ensure they are not left inactive. Implement a policy to revoke unused access keys. For services like AWS CLI, use roles or temporary credentials instead of long-lived keys. 
+ Temporary Suspend unused keys especially if an employee is on vacation for 2 weeks 
+ `Used groups as much as possible to manage user permissions`: Instead of assigning permissions directly to IAM users, create IAM groups and assign permissions to the group. Then, add users to the appropriate group(s). This simplifies permission management and ensures consistency.
+ `Avoid using wild card` 
+ `Set up strong password policies for your iam users this prevents poor passwords which could lead to compromise 
+ Set up IAM user password rotation 90days preventing IAM users from reusing the same password 
+ Enable Multi-Factor Authentication (MFA):Enable MFA on the root account and any other IAM user accounts . This adds an extra layer of protection to prevent unauthorized access.
+ `Use more of Managed Policies (Cusotmer Managed) where possible` : AWS provides predefined managed policies that cover common use cases and best practices. Use these policies when possible instead of creating custom policies, unless you have very specific requirements.
+ `Monitor and Audit IAM Activity`: Enable CloudTrail to log IAM activity and regularly review logs to track who accessed what resources and when. Use this data for security auditing and for identifying suspicious activities.Also use features like Credential report and Access Analyzers (external or unused accesses)





#################################
# EC2 INSTANCES  Elactic COMPUTE 
#################################

EC2 - Elastic Compute Cloud
EC2 - Core / Main  services in AWS 
EC2 - Regional Service / Zonal Service 
EC2 instance is the general name of how aws identifies its Compute its virtual machines. 


**PURCHASING OPTION**
Free Tier :   Free Trial 

750 hours 

**ON DEMAND INSTANCES** : You will create a virtual machine or EC2 instance without any long term commitment, no upfront payment.You pay as you go and its flexible.This is purchasing option is `expensive` when compared to other purchasing options in aws . 

## Use case : 
+ application deployments especially applications with unpredictable workload 
+ Developments , Testing , production env etc 

**RESERVED INSTANCES** : It gives you signifcant savings discounts for up to %75 when compared to on demand instances. you can reserve an instance for 1 year , 2 years or 3 years in particular aws region , and a specific instance type . You can pay upfront , you can make partial payment or no upfront payment . Its not flexible like on demand instances. 

+ *USE CASE* : Good for applications with predictable workload 

**SPOT INSTANCES** : This are instances that aws allows you to place a bid to unused EC2 capacity and you will be able to run the instances as long as your bid exceeds the current spot price . Spot instances and thier prices are set up by AWS and these prices will fluctuate based on demand for the instances . AWS offers 90 % off  demand rates .
+ However even though its cost savings , cheaper the instance can be interrupted by AWS with a two minute warning when the capacity is needed for on demand instances . 

## USE CASE : Temporary task , applications that are flexible , fault tolerence , handle interruptions ,data analysis . 


## SAVING PLANS :
It offers significant savings and more flexible than reserved instance .You can reserved an instance for 1-3 years .You will commit to consistent amount of usage ($/hr) for 1 year or 3 year term . $0.53/hr .With the saving plans it offers lower price on EC2 instances regardless of the instance family , size , OS , tenancy , aws region in exhange the above commitment of usage.

+ Compute Savings Plan : 
+ EC2 instance saving plan : Offers lower prices on a specific instance family in a choosen aws region . 


## DEDICATED HOST:  
A dedicated host is a pyshical server with EC2 instance capacity fully dedicated to your use . Dedicated host allows you to use software licenses and can help you meet compliance requirements. Unlike  EC2  that will run on a shared hardware , dedicate hosts gives visiblity and control how instances are placed on the server. 
You can install your own software like windows servers , Linux  etc

`Use Case` : scenario where you what EC2 instances to be physically isolated at the host hardware level from instances that belong to other AWS accounts.When a company is subject meet certain policies or regulatory . 

Pricing : varies based on instance family , region and payment option you choose . You can pay to own the entire host which is expensive but on the other hand  dedicated instances are cost effective 



## DEDICATED INSTANCE 
 A dedicated instance are EC2 instances that are physically isolated at the host hardware level from instances that belong to other aws account (customers). However unlike dedicated hosts you do not have control over instance placements and cannot use your software existing license 

 + Benefit is to enhance security by ensuring strict isolation. 
 + It can be for regulations , compliance 


## CAPACITY RESERVATIONS :
This is when you can reserve a number of ec2 instances in a specific availability zone for any duration . 
USE CASE : This is vital when you always want the capacity needed for your critical application .
The price here is charged based on the on demand rates . 


Summary 

+ `Flexibility` :  On demand instances
+ `Predictability , Long term usage` : Reserved instances and Saving plans 
+ `High cost savings and flexible workloads` : Spot instances 
+ `Compliance and License requirements` : Dedicated Hosts or Dedicated instances
+ `Guaranteed Capacity` :  Capacity reservations 


# Interview Question 

1. Can you tell me how you have helped your company to implement cost optimization in the cloud e.g AWS 



**INSTANCE TYPE** 



+ **General Purposes** : General purpose instances provide a balance of compute, memory, and networking resources. These instances are ideal for applications that use these resources in equal proportions, such as web servers and code repositories. e.g T2 - t2 medium , micro etc 

+ **Compute Optimized** : Compute optimized instances are designed for compute intensive applications that benefit from high performance processors. These instances are ideal for batch processing workloads, media transcoding, high performance web servers, high performance computing (HPC), scientific modeling, dedicated gaming servers, ad server engines, and machine 
learning inference.  e.g C family 

+ **Memory Optimized** : Memory optimized instances are designed to deliver fast performance for workloads that process large datasets in memory. e.g R family 

+ **Storage Optimized** : Storage optimized instances are designed for workloads that require high, sequential read and write access to very large data sets on local storage. They are optimized to deliver tens of thousands of low-latency, random I/O operations per second (IOPS) to applications. e.g D FAMILY 

+ **Accelerated Computer instance**: Ac
celerated computing instances use hardware accelerators, or co-processors, to perform functions, such as floating point number calculations, graphics processing, or data pattern matching, more efficiently than is possible in software running on CPUs.

+ **High Perfomance Instances** : High-performance computing instances are purpose built to offer the best price performance for running HPC workloads at scale on AWS. These instances are ideal for applications that benefit from high-performance processors, such as large, complex simulations and deep learning workloads.e.g  (HPC)


reference :  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html


EC2 ---> MAC 
OS --> Linux , Microsoft OS , MacOs 
Different Types of Linux Operating Systems (`Linux Distributions or Linux Flavours`  ) --->(Ubuntu , Redhat , CentOs , Fedora , Amazon Linux etc )



########################
# Thursday 13 ,2025
########################

`Launch instances` in aws 
`Provisioned` Ec2 instances in aws 
`Instanstiate` 


##  AMI : Amazon Machine Image 

Its a pre-configured template (file) that contains software configurations (Operating System, Application server , applications and related configurations or dependencies) required to launch a virtual machine ( EC2-Instance )

## WHAT YOU SHOULD KNOW WHEN IT COMES TO AMI 

+ Template for your ec2 instance. This is the Pre-configured OS(operating system) or application for your virtual machine 
+ Users can customize AMI to match their specific needs and they can also modify existing AMI 
+ You can use a single AMI to create multiple ec2- instances with the same AMI configurations
+ There are Private (only the user who owns the AMI can see and use it ) and Public AMI (anyone on the internet (aws) can see and use it) 
+ AMI can be build from a snapshoot 
+ AMI are built in specific regions (regional) , and can be copied from one region to another and from one aws to another 
+ You can get AMI from AWS market place


+ Private AMI 
+ Custom default AMI from AWS 
+ AMI from AWS market place ( Public AMI)


**KEY PAIRS** 

What are the different ways a user can login into an ec2 instance ? 
a use can be authenticated to login into an ec2 instance using the following methods : 

+ Key Pairs 
+ Password 

The process of logging is using key pairs and password is called SSH (Secure Shell )

+ A key pair consist of a public and a private key .The Public key is stored in AWS .While the private key is stored or kept by the user .Key pairs are primarilly used for SSH Access. 
+ An ec2 instance without a keypair you wont be able to SSH into the server but you can still login to the server using different methods . 
+ You can use a single keypair on several ec2 instances and you can use different keypairs on different ec2 instances 
+ When naming a keypair use a single name , do not put spaces in the naming convention 
+ When you create a keypair for the first time it goes to your downloads 
+ To be able to login you need to change the key permissions and you need to be in the location where the key is download or found 
+ To login into an EC2 instance through ssh security inbound rule port 22 must be open . The common troubleshooting error you will see when its connectivity issues like ports `Operation timed out` 


**Other methods of logging into an EC2 instance without using SSH** 

+ **EC2 instance connect** : 
The ec2 instance must be in the public subnet and has a public ip address and use a user name 
the default username is Ubuntu if you are using an ubuntu server .



Provisiong our first instance 

+ Give good naming conventions based of the software running in the computer  (e.g webservers , jenkns , sonarqube, database ) because it helps to differentiate what workload is running in the servers 

+ Next Select An `Amazon Machine Image ( AMI)`
+ Select Instance Type 
+ Create a Key pair : We can login into our ec2 through two authentication methods ( Keypairs or Passwords , ). When you create a key pair it will download the key to your download folder on your computer . 
+ 


`cases for ami in aws` 

+ we can use it to Launch new instances 
+ we can use AMI for automation and scaling 
+ We can use AMI for version control 
+ We can use to migrate an ec2 instance from one region to another and one aws account to another 
+ We can use AMI for backup and disaster recovery


+ Private AMI 
+ Custom default AMI from AWS 
+ AMI from AWS market place ( Public AMI)




How to login into AN ec2 instance 

+ Locate the instance you just created by click EC2 and click `instances running` 



**KEY PAIRS** 

What are the different ways a user can login into an ec2 instance ? 
a user can be authenticated to login into an ec2 instance using the following methods : 

+ Key Pairs 
+ Password 

The process of logging in an EC2 instance using key pairs and password is called `SSH (Secure Shell )`

+ A key pair consist of a public and a private key .The Public key is stored in AWS .While the private key is stored or kept by the user .Key pairs are primarilly used for SSH Access. 
+ An ec2 instance without a keypair you wont be able to SSH into the server but you can still login to the server using different methods or approach . 
+ You can use a single keypair on several ec2 instances and you can use different keypairs on different ec2 instances 
+ When naming a keypair use a single name , do not put spaces in the naming convention 
+ When you create a keypair for the first time it goes to your downloads 
+ To be able to login you need to change the key permissions and you need to be in the location where the key is downloaded or found and the command is `chmod 400 <name-of-keypair> this can be found under downloads or aws when you click on connect on the ec2 instance and select SSH 
`Note` : When you login into your ec2 instance succesfully for the first time meaning you change the permission of the key and was able login , for your next logins you do not need to run the permission command anymore . 
+ To login into an EC2 instance through ssh security group inbound rules for port 22 must be open . The common troubleshooting error you will see when its connectivity issues will be`Operation timed out` 


**Other methods of logging into an EC2 instance without using SSH** 

+ **EC2 instance connect** : 
The ec2 instance must be in the public subnet and has a public ip address and use a user name 
the default username is Ubuntu if you are using an ubuntu server .
You can also connect to the private subnet using same approach if you have an instance connect endpoint 

+ **Session Manager** : 

with session manager you can connect to your ec2 instance without : 
+ Connect to your instance without SSH keys, a bastion host, or opening any inbound ports.
+ Sessions are secured using an AWS Key Management Service key ( KMS- its used to encrypt data).
+ You can log session commands and details in an Amazon S3 bucket or CloudWatch Logs log groups.
+ You must attached an IAM role in other for your ec2 instance to have permission to use the SSM service in aws the permission you need to attach are (`AmazonEC2RoleforSSM` AND `AmazonSSMManagedInstanceCore` ,).
+ Note you can use a single SSM role to Many EC2 INSTANCES at same time
+ To update your ec2 with the IAM role you have created , click on instance -----> click actions -----> select security ----> click modify IAM Role ---> select the IAM role you create and save . 
+ you can access your ec2 instances both in private and public subnets using this approach 

+ **EC2 SERIAL CONSOLE** 
+ **SSH (SECURE SHELL)**  : you will use keypairs and passwords for authentication & port 22 must be open .
+ **RDP(Remote Desktop Protocol)** : This logs into the server which gives a graphical user interface for your Server 

STEPS TO SETUP YOUR RDP connection 

+ Install update your server `sudo apt update`  
+ install xrdp sererv : ` sudo apt install xrdp`
+ Start the xrdp service and enable it so that when we reboot our system it will be running ` 


You connect to your EC2 instances and aws resources using `CLOUDSHELL` 

+ Cloud shell is an integrated ,browser based shell environment that provides you the ability to run command line (CLI) to manage your aws resources.It allows you run commands , scripts , to interact with your aws services without you installing any software . 
+ Session Manager 
+ EC2 serial console 



**Security Groups :**  regional resources 

+ Security are what we call Virtual Firewalls . This controls the network traffic in and out of your ec2 instance called inbound and outbound traffic.
+ This is security at the instance level 
+ By default when you create an instance it comes with a default security group 
+ You need port 22 inbound rule open on your security group  before user can ssh into  your ec2 instance
+ A security group must be attached to a VPC (virtual private cloud)
+ You can attach a single security group to multiple ec2 instance within the same Region 
+ When you configure a source like 0.0.0.0/0 : it means you are allowing access from anyone and anywhere .This is not recommended and not a good security practise 
+ apart from using the source as ip address cidr like  (`0.0.0.0.0/0 or 123.495.697/32`) you can also use configure the source as a security group id `sgr-08adaec2ddb600c5f`
+ Security groups are stateful in nature If it allows inbound request or traffic into your ec2 instance , it automatically allow outbound request without setting any outbond rules . ***
+ When you create a security by default it denies  all inbound traffic and allow all outbound traffic .


source ip :  Where is the traffic coming from ?
destination ip : where is the traffic going to 

Interview Question 

+ How do you ensure your resources in the cloud are secured 
+ Can you tell me your experience with cloud network security 


**Best Practise when configuring a security group** 

+ Ensure you follow the principle least previllage 
+ Avoid granting access to sources like 0.0.0.0 /0 grant access only to trusted ip address or security groups
+ Have one security group just to manage SSH connections 
+ Monitor security groups regulary (Audit)


**What is the difference between a security group and a NACL (Network Access control list)**

security groups are stateful while nacl is stateless
security group is firewall at ec2 instance level while NACL is firewall at subnet level 

*Why should we set outbound rules if by default security groups are stateful ?* 

+ For restricting outbound traffic to pariticular sources 
+ to prevent Data exfiltration . that is allowing sensitive data to be send out to bad actors . 
+ for compliance purposes , if your company needs more restriction for inbound and outbound traffic at instance level.

**HOW T SSH FROM INSTANCE TO ANOTHER USING SECURITY AS SOURCE*




**EBS VOLUMES** 

EBS - Elastic Block Store it is scalable , high perfomance block storage .This is compared to a hard drive on a physical computer 

When you create a Virtual Machine (EC2) it comes with an EBS volume .The full acronyme for ebs is `Elastic Block Store` 


**Features of an EBS volume** : 

+ *Persistent Storage* : The EBS volume can persist data regardless of the life cycle of the ec2 instance. that is if you shutdown the instance and restart you wont lose data. 
+ *Easy to scale* : you can increase the size of an ebs volume , create new ebs volumes and attach to ec2 instances (Vertical scale and Horizontal scaling)
+ *Highly available & Durable** they can be replicated across different AZ (data center)
+ *You create snapshots* Snaps shots are simply a replica of the current state of your ebs volume which can be used for point-in-time recoveries.You can use snapshots to backup data OR  migrate data .When you take snapshot of your EBS its stored in an S3 bucket.
snapshots are incremental backups , this means only the blocks (data) that was not previously captured in the snaptshot will be captured.
+ Perfomance Options : EBS has different types of options you can select base on the storage perfomance for e.g
(General Purpose SSD, Provisioned IOPS SSD , Throughput Optimized HDD , Cold HDD , Magnetic Standard) 




**VOLUMES :** 
**Different Types of EBS VOLUMES** 

**General Purpose SSD (gp2 ,gp3)** : 
This is aws trying to give you a storage that can use for multi purpose use by balancing it with price and perfomance 
It is good with different varieties of workloads ,and applications and gp3 volumes (storage) offer high performance and lower cost 
than gp2. 
+ Cost effective and has low latency 
+ Its good for system boot volumes , virtual desktops , development and test environments

**Provisioned IOPS SSD (io1 ,io2) :** 
This EBS volume type is designed for I/O -Intensive applications such as Datases because databases need persistent , consistent and higher IOPS. io2 volumes it offers higher durability than io1.

+ IOPS  means `input and output Operations per second` 

**Throughput Optimized HDD (st1**): Its designed to handle frequenly accessed throughput intensive workload . Low cost e.g Data warehousing , log processing 


+ Throughput : in the context of computer networking and storage , This is when data is succesfully transfered from one location to another and its measured in bits per second (bps ), Mbps (mega-bits per second) , gigabits per second. 


**Cold HDD (sc1)** : This storage is designed for less frequently accessed data.
It provides low cost per GB . This is good for scenarios where price is more of a concern than perfomance . 
e.g its good to save backup files or files serves in ec2 . 

**Magnetic Standard** : Its the previous generation of the HDD .This EBS type is not recommended due to its lower perfomance and its high cost when compared to other new ebs storages . 




**Steps to Increase an EBS Volume

**Step 1: Identify the EBS Volume**

Find the EBS Volume ID attached to your instance or locate through your EC2 instance under Storage 
Verify the current size of the volume.


**Step 2: Modify the EBS Volume**

You can modify the volume via AWS Console

Option 1: Modify EBS Volume via AWS Console

Open the EC2 Dashboard.
Navigate to Volumes under the Elastic Block Store section.
Select the EBS Volume you want to resize.
Go under actions and select Modify Volume
Enter the new size (GB) for the volume.
Click Modify, then Confirm.



**Step 3: Extend the Filesystem**
On the commandline run the following commands

`lsblk`
`sudo growpart /dev/xvdf 1`  # Replace xvdf with your disk name
`sudo resize2fs /dev/xvdf1`  # Replace xvdf1 with your partition






#####################
EBS SNAPSHOTS 
#####################

All ec2 instances comes with a root volume (EBS)

*Root Volume* : The root volume is refered to as the primary storage device attached to a virtual machine (ec2 instance)
It contains the operating system ,system files and any other dependencies that requires the instance to function 

*Data Volume* : This is refered to the storage volume primarily used for storing application data , user data , and any other type of non system data 

`What is a Data`a : This is any information that can be stored e.g name can be data , files can be data . Data can be classified ( Sensitive (only authorized users can have access to it ) or non sensitive data (anyone can see or have access to it ). 

What are snapshots : This is a copy or replica of your EBS Volume .

By default when you create an aws account , ec2 ebs encryption is turned off . you will need to manually turn it on .
What happen if you have already created EBS volumes with encryption not turn on . 

What is Encryption ? : This is simply turning plain text into cipher text. This is when you mask or hide sensitive data from non authorized users using what we call encyrption keys 

+ Encryption at Rest : This is when the data is stored in a storage and not moving and its secured using encryption keys
+ Encryption in flight : When the data is moving from one location to another 





**HOW CAN YOU ENCRYPT AN EXISTING EBS VOLUME THAT HAS NOT BEEN ENCRYPTED** 

+ aws offers encryption service through KMS (Key management service ) this service offers encryption at rest 
it has default kms key or you can create a custom kms key .
+ Certificate Manager (offer data encryption in flight ) SSL /TSL certificate 

**STEPS TO ENCRYPT AND UNECRYPTED EBS VOLUME :** 

+ Identify the ebs volume that has not been encrypted 
+ Create a snapshot from the existing ebs volume (if the ebs volume is attached to a running instance)
stop the ec2 before creating a snapshot) . Note you can still create the snapshot while the instance is running. To create the snapshot click on selected ebs volume and click on actions and create a snapshot . 
+ Navigate to snapshots and locate the new snapshot that you have created 
+ Create a volume from the snapshot that you created .go on action and `select create volume from snapshot` when you are creating the new volume you can now encrypt the new volume. 
`Note` : The new ebs volume you are creating make sure its located in the same Availability zone as the ec2 instance where you need to attached the ebs volume to 


Another method of encyption :

+ create a snapshot of the ebs volume
+ create a copy of the snapshot and while creating the copy of the snapshot ,encrypt the copy 
+ Create a volume from the encrypted ebs snapshot and the volume will be encrypted 




**What are they advantages of taking snapshots or the feature in ebs volume**

Snapshots can be used for various purposes : 

+ Data protection and point in time recovery 
+ Data Migration 
+ Compliance and Auditing 
+ System Maintenance : Its good to take snapshots when doing system upgrades so you can easily roll back 
+ Backup Data

Data migration : 

+ we created a snapshot from an existing ebs volume 
+ from the snapshot we created a volume 
+ we attached the volume to a new ec2 instance with the data inside the volume 



# Life CYCLE Manager : 

+ This is feature that helps you to automate the creation of EBS snapshots.
+ You can create a life cyle policy to automatically take snapshots on your behalf at specific time 
+ You can create a custom policy and or a default policy .
+ For custom policies you can create an `EBS SNAPSHOOT POLICY` ,`EBS AMI BACKED CUSTOM POLICY` , `CROSS ACCOUNT COPY EVENT POLICY` 

+ EBS SNAPSHOOT POLICY : This policy creates EBS SNAPSHOTS on your behalf 
+ EBS AMI BACKED CUSTOM POLICY: Its going to create  AMI for your instances on your behalf 
+ CROSS ACCOUNT COPY EVENT POLICY` : It copies EBS or AMI across your aws account to another account 


HOW CAN WE COPY AN AMI FROM ONE AWS ACCOUNT TO ANOTHER (Customer KMS )
EFS ( ELASTIC FILE SYSTEM ) 



**Default Life Cyle Policy** 
+ With this policy you can exclude ebs volumes that you dont want to take snapshots using tags 

**Custom Life Cyle policy `EBS Snapshots Policy`**
+ you have to setup Tags for the resources e.g EBS or instance that you want the policy to target and create snapshots


##########################
Monday July 29 ,2024
##########################

Copy AMI or EBS volume from one AWS region to another 

**HOW CAN WE COPY AN EBS VOLUME or AMI from AWS account to another**  

**AMI* : 
+ Create an Image from your existing EC2 instance (select instance , click on actions , and select create image )
+ On left bar ,click on AMI's and select the image you created ( click actions and select `modify permissions` )
+ Paste the account you want to copy the AMI To . 
+ Note : You can share AMI from one aws account to another if the EBS volume is not encrypted .you can share without any issues but if the EC2 instance that you created the AMI image from has an ebs volume that is encrypted with Default KMS key or a custom KMS key that does not grant permission to the destination account you wont be able to share the AMI . 



What can cause you to migrate AMI from one aws account to another or region to region 

+ Backup Data (High Avaialable)
+ Security reasons : standalone aws account ( production , stage , development )  


**EBS SNAPSHOT RECYCLE BIN**

The EBS Snapshot Recycle Bin is a feature that allows you to protect your EBS snapshots from accidental deletion. When you delete a snapshot, it is moved to the Recycle Bin, where it is retained for a specified period before permanent deletion. This gives you the opportunity to recover the snapshot if it was deleted by mistake. Here's how you can set up and use the EBS Snapshot Recycle Bin:

The snapshots recycle bin can store both ebs snapshots and ami images 

+ You will need to create a retentionr rule 
+ The retention rule is regional . 



SNAPSHOTS ---INCREMENTAL 

BREAK : 9: 25 PM EST 




**EBS SNAPSHOT RECYCLE BIN**

The EBS Snapshot Recycle Bin is a feature that allows you to protect your EBS snapshots from accidental deletion. When you delete a snapshot, it is moved to the Recycle Bin, where it is retained for a specified period before permanent deletion. This gives you the opportunity to recover the snapshot if it was deleted by mistake. Here's how you can set up and use the EBS Snapshot Recycle Bin:

The snapshots recycle bin can store both ebs snapshots and ami images 

+ You will need to create a retention rule 
+ The retention rule is regional . 



Thursday Feb 20 


############################
# FILE SYSTEMS 
############################

**EFS : Elastic File System**

its is a cloud-based file storage service provided by AWS that offers a simple, scalable feature . elastic file system can be use with AWS Cloud services and `on-premises` resources. It's designed to provide scalable file storage that can be mounted on thousands of Amazon EC2 instances and is also accessible from on-premises servers, making it ideal for a wide range of applications and use cases. EFS is built to be highly durable and highly available, with data stored across multiple Availability Zones (AZs).

+ Its Fully Managed Service . ( Infrastructure management hardware , Operating system , networking ,it handles scaling (vertical or horizontal scaling), Patch Management , Backup & Recovery , Monitoring )  

+ Scale performance 
+ Highly Available 
+ Its compactible with Linux based Operating systems . It does not support windows based operating systems 
+ Cost Effective 
+ Secured 
+ You can connect your EFS from aws cloud to on premise using VPN or Direct Connect (transit)
+ You can mount multiple EFS  system on a single ec2 instance 
+ You can create multiple mount point from a single EFS file system 


**USE CASE** : 

+ Web services and content management 
+ backups 
+ Data migration ( migrate data from on premise to cloud and vice versa )
+ Applications 


**EFS (ELASTIC FILE SYSTEM) STORAGE CLASSES** 

It offers multiple storage classes . 

1. **Standard Storage Classes**
+ **EFS Standard** : This storage class is designed for frequently accessed files 
+ **EFS Standard-infrequent access (Standard-IA)** : Its designed for files that are not frequently accessed .
EFS automatically moves files that haven't been accessed according to the lifecycle policy (usually 30 days) to the Standard-IA storage class to reduce costs while still ensuring data is accessible without retrieval delays. This class charges for data retrieval, so it's best suited for infrequently accessed data. 




2. **One Zone Storage Classes**  (data is stored in a single availability zone )
**EFS one Zone** : This storage classes allows you to store your data in a single availability zone and this data stored in this single availability zone will be frequently accessed.

**EFS one zone infrequent Accessed (one Zone IA)**: Data is stored in a single availability zone and the data is not frequently accessed . It offers a lower storage cost when compared EFS one zone and the EFS standard IA . 

+ with on zone storage class you can not transition your files or data to Archive storage 

+ High Available storage ?
+ Cost Optimization 
+ Durable access pattern 
+ Scale (workload unpredictable)



**Lifecycle management for EFS**

Transition into Infrequent Access (IA):
30 day(s) since last access
Transition into Archive:
90 day(s) since last access
Transition into Standard:
None


**SETTING UP EFS ( ELASTIC FILE SYSTEM)**

When creating an EFS you will need to select the performance mode for your efs in regards througput .
Throughput in the context of  computer networking and storage , This is when data is succesfully transfered from one location to another and its measured in bits per second (bps ), Mbps (mega-bits per second) , gigabits per second. )
we will need to select either the `enhanced or bursting mode` .
Within the enhanced mode we have the Elastic or Provisioned 


**Perfomance Settings** :

`Different Throughput modes` 

+ `Enhanced` :    ( Elastic and Provisioned )

+ `Bursting Mode`  : Default throughput mode where performance scales with the size of the file system.Provides baseline throughput with the ability to burst up to higher levels.


**File system policy** 

These are policies you can set to enhance security on your EFS and some of the policies you can set up are : 

+ Prevent root access by default*
+ Enforce read-only access by default*
+ Prevent anonymous access
+ Enforce in-transit encryption for all clients



EFS Security group I.D   :   sg-038bd7ba01c7f3f40


EFS mount point: /mnt/efs/fs1

**Steps to Create EFS** 

+ We created an EFS file system and we should the Throughput Enhance - Elastic 
+ While creating the file system we created a custom security group and attached to the EFS . 
+ We created 2 ec2 instances with the same security groups 
+ We going to edit the the security groups ( EFS security group inbound rules and EC2 instances Inbound rules) to accept Port  NFS PORT : 2049


**Difference Between EFS and EBS volume** 

+ Ebs provides a block level storage vs EFS offers file level storage which allows multiple ec2 instances to access the same file simultaneouly 
+ Scalability : Ebs volumes needs to be scaled manually  but with EFS you can scale manually and automatically . 
+ Attachment and mounting; for ebs volumes you can attach to only one instance at a time . Multi Attach enabled ebs volumes (oi and o2 types) and with this you will only be able to attach in a single availability zone 
while EFS you can mount on multiple ec2 instances at the same and different availability zones. 
+ Backup & recovery. Ebs uses snapshots which can be used for point in time backup of volumes.while EFS you can enable automated backups this is managed by AWS. 
+ Cost and provisioning .EBS requires users to provisioned (that which storage capacity you want) when you do this you have anticipated the storage you will need. On other hand EFS will scale in or out automatically if demand increases ir decreases 



AMAZON FSX


**AMAZON FSX** (`Fully managed third-party file systems optimized for a variety of workloads)`
 Its a file storage service offered by aws and its fully managed service and its to launch and run popular file systems with good performance ,security . It can be used for different use cases .FSx Provides fully managed third party file systems for for native compatibility (match) and features .

 Amazon FSx is a fully managed service that provides file storage optimized for a wide range of workloads. It enables organizations to deploy and manage shared file systems in the cloud, integrating seamlessly with other AWS services. FSx supports multiple file system types tailored to different performance and compatibility needs.


**DIFFERENT FILES SYSTEMS UNDER AMAZON FSx** 

 - FSx for NetApp Ontap
 - FSx for OpenZFS
 - FSx for Windows File Server
 - FSx for Lustre 

 ### FSx for Windows File Server

 - This is a fully managed windows file system share drive 
 - it supports SMB protocol and windows NTFS
 - It supports Microsoft Active Directory Integration , ACL , user quotas
 - it can be mounted on a linux EC2 Instance 
 - you can scale this file system up to 10 s of GB 
 - You connect it to your onpremise network using a VPN or Direct Connect 
 - It is highly available because you can configure to be in different availability zones ( data centers)
 - You can back up the data daily into an s3 bucket 


# USE CASE 

Hosting a business application , if you require a workflow that requires a windows file storage .

## FSx for Lustre 

- The name lustre is taken from the word  linux - cluster (lustre )
- This is a fully managed file system that is optimized for high performance computing , machine learning and media processing workloads. 
it can handle large amount of data processing workloads .
- it can scale up to 100s GBS/S , millions IOPS
- it has different storage options ( SDD , HDD)
- You can intergrate or connect with S3 
- You connect it to your onpremise network using a VPN or Direct Connect 

## FSx for NetApp ONTAP : 

- This service provides a managed NetApp ONTAP file system with full support for the ONTAP data management feature 
- It supports NFS . SMB and ISCSI 
- it is good for enterprize applications , databases , and storage for virtual machines (ec2 instances ) (VM)
- it works with the following operating systems 9 Linux , windows , MacOS , EC2 , EKS ,ECS, Vmcloud on aws 
- It can shrink storage or it can grow the storage automatically 
- Snapshots , replication , low cost , compression and data de-duplication

USE CASE : Disaster recovery, backup , virtualization 


### FSx for OpenZFS

This service provides a managed file system that provides high performance and rich set of data management capabilities 
- it supports snapshots 
- it supports cloning 
- it supports data compression
   all these makes it a use case for software development environments , data analytics application , media processing workflow 
- 1 ,000,000 IOPS with < 0.5ms latency 



**Advance Settings (Details) for OUR EC2 instances**



+ **Domain join directory*: This is when you want your instance to have domain based authenticattion with a centralized authentication service like Active Directory  , SSO .

+ IAM instance profile  : This settings helps you to grant access to your ec2 instance using IAM roles 


What is a Hostname*: This is a label(name) assigned to your device (computer or server) on a network thats helps to identity the device in a human readable format .

+ **Hostname type* : This is a feature in EC2 when you want to automatically assigned a DNS name to your ec2 instance .It has ip name and resource name options . You can have a public and a private hostname

+ *Instance auto-recovery*: It helps to ensure availability and reliability of your instances by automatically recovering from any hardware failures or issues 

+ *Shutdown behavior* : This is an action an EC2 instance will take or receive when it receives a shutdown signal .You can configure your ec2 instance in such away how it will response when its been shut down . e.g stop , the instance will shutdown this is similar to when you turn off your compuer (you can start your instance later , ebs data is still intact , you wont incure any changes) but when you terminate (the instance will shutdown and will be deleted and this action is not reversible)

+ *Stop - Hibernate behavior* When the instance is shutdown it will hibernate ,This is going to save the instance RAM content to the EBS root volume , allowing your instance to resume in the exact location . You might hibernate an ec2 if you need to resume the instance faster . Also when your hibernate the aws will not charge you but you might incur charges for the RAM Content 

+ *Termination protection* :This is when you want to protect your ec2 instance from unintended Termination 

+ *Stop protection*This is when you want to protect your ec2 instance from unintended Stop .

+ *Detailed CloudWatch monitoring*: This will monitor your ec2 instance such as metrics like EBS volume etc . 

+ *Credit specification*: This is a settings that applies to Instances that uses CPU credit for bursting performance.e.g T3 ,T2 , T3A 

+ *Placement group* : This is a way you place your ec2 instances in the cloud to meet various aspects liker perfomance and availability requirements . 

+ **Kernel ID* : Select the kernel ID for the ec2 instance Architecture you prefer.
+ **License configurations* : This is when you have your own license and want to use for your EC2 instance to manage software applications that runs on the ec2 instance . 





**Placement Groups**

A placement group in AWS (Amazon Web Services) is indeed a configuration option that allows you to control how a set of interdependent instances (servers) are physically positioned within the AWS infrastructure. By using placement groups, you can set the deployment of these instances to meet specific requirements of your workload, such as an application, web server, or database, with considerations for aspects like low latency, high resilience, or high availability. 

Placement groups are particularly useful when your workload demands a high level of communication performance between instances, or when you have specific needs regarding the physical proximity(closeness) of your virtual servers (ec2).



1. **Cluster Placement Group** : A cluster placement group is a logical grouping of instances within a single Availability Zone. Instances in this group are placed close together to provide low latency and high throughput networking.

*Use case* : This is good for applications that require high network performance 

*Disadvantages of Cluster Placement Group   

+ Single availability zone 
+ Limited Instance Types. Some instance types may not be supported by the cluster placement groups


2. **Spread Placement Group** : A spread placement group distributes instances across multiple underlying hardware racks within a single Availability Zone. This ensures that instances are placed on separate racks to minimize the risk of simultaneous failure.

*Use Case*: Suitable for applications that need to be highly available and resilient to hardware failures. It is often used for critical applications where you want to ensure that no two instances are on the same hardware rack.

**Limitation*

+ Limitation with the number of instance you can launch in a single AZ  you can have 7 instances per AZ though the number can change depending the Region.
+ Reduced network performance when compared to cluster placement group  .
+ Availability Zone Restrictions: While spreading instances can enhance availability, it also means that your application components might be spread too thinly across AZs, potentially impacting performance due to inter-AZ latency.


3. **Partitioned Placement Group :** :  A partition placement group divides instances into logical partitions, where each partition is placed on distinct hardware to avoid single points of failure. Instances within the same partition share the same hardware but are isolated from instances in other partitions.

USE CASE : Ideal for large distributed and replicated applications that require high fault tolerance, such as Hadoop, Cassandra, or MongoDB clusters.

*Limitations*

+ Complex to manage especially as you scale 
+ Limit on partition.There's a limit to the number of partitions you can have within a placement group, which can vary by region. This limit might restrict how you scale your application.
+ inter partition latency : While instances within the same partition benefit from low-latency connections, communication between instances in different partitions might experience higher latency, as they could be located in different physical areas within a data center




**USER DATA*

Amazon EC2 offers a feature where you can do a pre-configuration of your EC2 INSTANCE at launch time . you can install packages using a bash shell script . This process is know as `Bootstrap` .

User Data in Amazon EC2 (Elastic Compute Cloud) refers to the script or data that is provided by the user when launching an instance. This data is used to perform automatic configurations or run scripts on the instance as it boots up for the first time. User data can include shell scripts, cloud-init directives, or other automation data that applies to your instance to configure software packages, install applications, manage files, or perform other bootstrapping tasks.

example : 

#!/bin/bash 
sudo su - 
apt-get update -y 
apt-get install apache2 -y
systemctl start apache2
systemctl enable apache2
echo "<html><h1>App2: Welcome to TEAM4TECH SOLUTIONS CLASS8 USER DATA DEPLOYMENT</h1></html>" > /var/www/html/index.html


+ Static web application (content does not change user must manually update the content )
+ Dynamic web application (content changes all the time . e.g facebook , amazon )
+ Standalone web application : This is a web application that uses the users Operating system to work without any webserver and it does not need internet to function e.g Microsoft word 


U CAN USE USER DATA FOR MULTPIPLE 

+ Software installations or package installations
+ Application deployments ( static websites )



**LOAD BALANCING**
Under EC2 instances we have a feature called Load Balancing 

Load Balancers 
Target groups 

What is a Load Balancer : ?

This is a device or software that helps to distribute your network or application trafic across multiple devices.The goal is to ensure no single server(ec2 instance) should be overwhelmed with too much traffic , when your server is not overwhelmed with too much traffic this improves your application perfomance and reliability.


## HOW DOES A LOAD BALANCER DETERMINED WHEN AN EC2 INSTANCE (WEB SERVER) HAS LESS LOAD BEFORE IT ROUTES TRAFFIC TO IT . 


A load balancer will monitor your servers : 

1. **Perfomance Metrics** : Cpu usage , Memory , Responsive Time ,these metrics gives the loadbalancer an insight on how much load a server has . 

2. **Round Trip Time** : Some load balancers measure the round trip time ,which means a longer round trip time indicates the servers might be experiecing more workload .

3. **Load Balancing Algorithms** :  Load balancers uses Round-Robin algorithm to distribute request equally regardless of the load. it uses both the health check and perfomance metrics

**There other alorithms** : 

These are features you can set on your application loadbalancer to control it on how it distributes traffic to your backend servers.

+ *Round Robin*: Distributes traffic evenly across all healthy instances in a rotating order.
+ *Least Connections*: Directs traffic to the instance with the fewest active connections.
+ *Least Response Time*: Sends traffic to the instance with the quickest response time.
+ *Weighted Round Robin*: Assigns more traffic to instances with greater capacity or better performance, based on predefined weights.
+ *IP Hash*: Routes requests based on a hash of the client’s IP address, ensuring consistent routing for the same client.


4. **Health Checks** :   They check the server to make sure they are available and functioning well . It uses ping to check if the server is responding as expected . servers will respond with HTTP 200 OK 
*Ping*: Sending a simple network ping to see if the instance is responsive.
*HTTP/HTTPS Request*: Making a request to a specific URL and checking for a healthy response (e.g., HTTP 200 OK).
*TCP Connection*: Attempting to establish a TCP connection to a specific port.

5. **Timeout and Error Rates** : It monitors the request timeout and if the load balancer receives a response with timeout it will determined if there is a an issue with the server .


## DIFFERENT TYPES OF LOAD BALANCERS 

1. Application Load Balancer (Layer 7 )
2. Network Load Balancer (Layer 4 )
3. Classic Load Balancer (does not recommend the use . old version) (supports layer 4 & 7 )
4. Gateway Load Balancer (Layer 3)


**What are Layers in Networking**

Networking has what we call layers  

Layers are categorized in different levels 
The OSI (Open Systems Interconnection) model is a conceptual framework used to understand and implement networking protocols in seven distinct layers. Each layer serves a specific function and communicates with the layers directly above and below it. Here's a brief overview of each layer and their roles:

**Layer 1: Physical Layer**

Function: Deals with the physical connection between devices, including cables, switches, and other hardware.
Key Elements: Electrical signals, cabling, connectors, network interface cards (NICs).
Example: Ethernet cables, fiber optics, physical ports.

**Layer 2: Data Link Layer**

Function: Provides node-to-node data transfer and handles error detection and correction from the physical layer.
Key Elements: MAC addresses, switches, frames.
Example: Ethernet, Wi-Fi (802.11), MAC address.

**Layer 3: Network Layer**

Function: Manages the routing of data packets between devices across different networks.
Key Elements: IP addresses, routers, packets.
Example: IP (Internet Protocol), ICMP (Internet Control Message Protocol).

**Layer 4: Transport Layer**

Function: Ensures reliable data transfer between devices, providing error checking and data flow control.
Key Elements: Ports, TCP/UDP protocols, segments/datagrams.
Example: TCP (Transmission Control Protocol), UDP (User Datagram Protocol).

**Layer 5: Session Layer**

Function: Manages sessions or connections between applications. It establishes, maintains, and terminates connections.
Key Elements: Sessions, dialogues, connections.
Example: NetBIOS, RPC (Remote Procedure Call).

**Layer 6: Presentation Layer**

Function: Translates data between the application layer and the network. Handles data encryption, compression, and translation.
Key Elements: Data formatting, encryption/decryption, compression.
Example: SSL/TLS (Secure Sockets Layer/Transport Layer Security), JPEG, ASCII.

**Layer 7: Application Layer**

Function: Provides end-user services and interfaces directly with applications. It facilitates communication between software applications and lower layers.
Key Elements: Application protocols, interfaces.
Example: HTTP (Hypertext Transfer Protocol), FTP (File Transfer Protocol), SMTP (Simple Mail Transfer Protocol), DNS (Domain Name System).

Understanding Layer 7 (Application Layer)
Layer 7 is the top layer in the OSI model and is closest to the end-user. This layer interacts directly with software applications to provide communication functions. It's responsible for:

Protocol Implementation: Ensuring that network services are available to applications (e.g., web browsers, email clients).
Data Formatting: Converting data into a format that applications can understand.
User Interface: Providing network services directly to end-user applications.
In the context of load balancing, a Layer 7 Load Balancer (like AWS Application Load Balancer) can make routing decisions based on:


**DIFFERENT TYPES OF LOAD BALANCERS** 

1. `Application Load Balancer ( uses layer 7)`

+ This is good  loadbalancer to route traffic to your web applications 
+ Application load balancers are designed for http (hypertech transfer protocol) and https (hypertech transfer protocol secured) traffic 
+ It operates under layer 7 ( Operates at the OSI (Open system interconnection) Model Layer 7 )
+ Application loadbalancer supports features like SSL/TSL terminations ,websockets and HTTP/2 
+ Application loadbalancers are good for modern application archectures (micro service , monolithic applications)
+ Applications loadbalancers can be internet facing or internal facing . 
+ Load Balancers comes with security groups to enhance security 
+ Load balancers will reach instances through what we call a Target group . A target group  helps a load balancer to target specific resources base on your configurations .



**Target Groups** : 

This is a logical grouping of your endpoint (target) which can be an ec2 instances (servers) where you want your load balancer to distribute traffic to . They can route traffic to one or more registered targets

**Different Target Types**

+ Instances 
+ Ip addresses
+ Lambda functions 
+ Application Loadbalancer 

Creating our first load balancer 


+ 3 ec2 instances and each of them we will deploy a simple static web application using apache 
The reason for creating this instances is because we want to demonstrate how our load will be routed when we create a load balancer 

+ Create a Target group (for unsecured traffic select http port 80 and for secure port 443)


9:05 pm est -  9:16 pm est 


http : This is is unsecured traffic and the port is 80 
https : This is secured traffic ( data in transit or flight is encrypted) port 443 



In aws we have a service called AWS Certificate Manager ( ACM ) and we will use this to encrypt our load balancer on port 443 which is https .





